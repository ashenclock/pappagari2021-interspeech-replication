===== ./src/models.py =====
import torch
import torch.nn as nn
from transformers import AutoModel
from speechbrain.pretrained import EncoderClassifier
import sys

class BertClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = AutoModel.from_pretrained(config.model.text.name)
        self.dropout = nn.Dropout(config.model.text.dropout)
        num_classes = 2 if config.task == 'classification' else 1
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
        nn.init.xavier_uniform_(self.classifier.weight)
        if self.classifier.bias is not None:
            nn.init.zeros_(self.classifier.bias)
            
    def forward(self, batch):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        if self.classifier.out_features > 1:
            return logits
        else:
            return logits.squeeze(-1)

class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = EncoderClassifier.from_hparams(source=config.model.audio.pretrained)
        
        if not config.model.audio.trainable_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = False

        last_layer = self.encoder.hparams.embedding_model.fc
        num_features = last_layer.conv.out_channels
        
        num_classes = 2 if config.task == 'classification' else 1
        
        self.classifier_head = nn.Sequential(
            nn.Linear(num_features, num_features // 2),
            nn.ReLU(),
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features // 2, num_classes)
        )

        for module in self.classifier_head.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, batch):
        # I dati arrivano dalla CPU, li spostiamo sul device del modello
        waveforms = batch['waveform'].to(next(self.parameters()).device)
        
        # Aggiungiamo la dimensione del canale
        if waveforms.dim() == 2:
            waveforms = waveforms.unsqueeze(1)
        
        # Bypassiamo la funzione problematica
        with torch.no_grad() if not self.encoder.parameters().__next__().requires_grad else torch.enable_grad():
             feats = self.encoder.mods.compute_features(waveforms)
             embeddings = self.encoder.mods.embedding_model(feats)

        embeddings = embeddings.squeeze(1) 
        outputs = self.classifier_head(embeddings)
        
        if self.classifier_head[-1].out_features > 1:
            return outputs
        else:
            return outputs.squeeze(-1)

def build_model(config):
    if config.modality == 'text':
        return BertClassifier(config)
    elif config.modality == 'audio':
        return EcapaTdnnClassifier(config)
    else:
        raise ValueError(f"Modello per la modalità '{config.modality}' non supportato.")
===== ./src/data.py =====
from pathlib import Path
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from sklearn.model_selection import StratifiedKFold
import torchaudio
from torchaudio.functional import resample

class TextDataset(Dataset):
    # --- QUESTA CLASSE È CORRETTA E RIMANE INVARIATA ---
    def __init__(self, df: pd.DataFrame, config, tokenizer):
        self.df = df; self.config = config; self.tokenizer = tokenizer; self.is_test = 'label' not in df.columns
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        transcript_dir = Path(self.config.data.transcripts_root if not self.is_test else self.config.data.test_transcripts_root)
        transcript_path = transcript_dir / self.config.transcription_model_for_training / f"{file_id}.txt"
        try: text = transcript_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError: text = ""
        inputs = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.config.model.text.max_length,
            padding="max_length", truncation=True, return_attention_mask=True, return_tensors="pt",
        )
        item = {"input_ids": inputs["input_ids"].flatten(), "attention_mask": inputs["attention_mask"].flatten(), "id": file_id}
        if not self.is_test:
            item['labels'] = torch.tensor(row['label'], dtype=torch.long if self.config.task == 'classification' else torch.float32)
        return item

class AudioDataset(Dataset):
    def __init__(self, df: pd.DataFrame, config):
        self.df = df
        self.config = config
        self.is_test = 'label' not in df.columns
        self.audio_root = Path(self.config.data.audio_root if not self.is_test else self.config.data.test_audio_root)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        audio_path = self.audio_root / f"{file_id}.wav"
        if not audio_path.exists() and not self.is_test:
            group = 'ad' if row['label'] == 1 else 'cn'
            audio_path = self.audio_root / group / f"{file_id}.wav"

        waveform, sr = torchaudio.load(audio_path)
        
        # --- FIX #1: FORZARE L'AUDIO MONO ---
        # Se la waveform ha più di un canale (es. è stereo), ne facciamo la media
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        # ------------------------------------

        if sr != self.config.model.audio.sample_rate:
            waveform = resample(waveform, sr, self.config.model.audio.sample_rate)
        
        # Ora .squeeze() funzionerà sempre, perché l'input è sempre [1, n_samples]
        item = {"waveform": waveform.squeeze(0), "id": file_id}

        if not self.is_test:
            target = row['label']
            dtype = torch.long if self.config.task == 'classification' else torch.float32
            item['labels'] = torch.tensor(target, dtype=dtype)
            
        return item

def collate_audio(batch):
    waveforms = [item['waveform'] for item in batch]
    ids = [item['id'] for item in batch]
    padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)
    collated_batch = {"waveform": padded_waveforms, "id": ids}
    if 'labels' in batch[0]:
        labels = torch.stack([item['labels'] for item in batch])
        collated_batch['labels'] = labels
    return collated_batch


def get_data_splits(config):
    """Prepara i DataFrame per training, validazione e test."""
    train_df = pd.read_csv(config.data.train_labels)
    train_df = train_df.rename(columns={'adressfname': 'ID', 'dx': 'diagnosis'})
    train_df = train_df.set_index('ID')
    
    if config.task == 'classification':
        train_df['label'] = train_df['diagnosis'].apply(lambda x: 1 if x == 'ad' else 0)
    else:
        train_df['label'] = train_df['mmse']
        
    # --- FIX #2: RIMOSSO .sort_index() ERRATO ---
    # Mescoliamo il dataframe una volta per sicurezza, senza riordinarlo.
    train_df = train_df.sample(frac=1, random_state=config.seed)
    # ---------------------------------------------
        
    test_df = pd.read_csv(config.data.test_task1_labels).rename(columns={'ID': 'id'}).set_index('id')

    skf = StratifiedKFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):
        train_split = train_df.iloc[train_idx]
        val_split = train_df.iloc[val_idx]
        yield fold, train_split, val_split, test_df
        
def get_dataloaders(config, train_df, val_df, test_df=None):
    if config.modality == 'text':
        tokenizer = AutoTokenizer.from_pretrained(config.model.text.name)
        train_dataset = TextDataset(train_df, config, tokenizer)
        val_dataset = TextDataset(val_df, config, tokenizer)
        collate_fn = None
    elif config.modality == 'audio':
        train_dataset = AudioDataset(train_df, config)
        val_dataset = AudioDataset(val_df, config)
        collate_fn = collate_audio
    else:
        raise ValueError(f"Modalità '{config.modality}' non supportata.")

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
    
    if test_df is not None:
        test_dataset = TextDataset(test_df, config, tokenizer) if config.modality == 'text' else AudioDataset(test_df, config)
        test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader, test_loader
        
    return train_loader, val_loader
===== ./src/config.py =====
import yaml
from pathlib import Path
from typing import Any

class Config:
    """
    Classe per caricare la configurazione da un file YAML e permettere
    l'accesso agli attributi tramite dot notation (es. config.data.audio_root).
    """
    def __init__(self, data: dict):
        for key, value in data.items():
            if isinstance(value, dict):
                setattr(self, key, Config(value))
            else:
                setattr(self, key, value)

    def __repr__(self):
        return str(self.__dict__)

    def to_dict(self) -> dict:
        result = {}
        for key, value in self.__dict__.items():
            if isinstance(value, Config):
                result[key] = value.to_dict()
            else:
                result[key] = value
        return result

def load_config(config_path: str | Path) -> Config:
    """Carica un file di configurazione YAML."""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"File di configurazione non trovato in: {path.resolve()}")
        
    with open(path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if not isinstance(config_data, dict):
        raise TypeError("La radice del file YAML deve essere un dizionario.")
        
    return Config(config_data)
===== ./src/utils.py =====
import torch
import numpy as np
import random
import gc

def set_seed(seed: int):
    """Imposta il seed per la riproducibilità."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def clear_memory():
    """Libera la memoria della GPU."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
===== ./src/engine.py =====
import torch
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, classification_report, confusion_matrix

from src.models import build_model
from src.utils import clear_memory

def train_epoch(model, loader, optimizer, scheduler, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in tqdm(loader, desc="Training", leave=False):
        optimizer.zero_grad()
        
        # Sposta il batch sul device corretto
        #batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        #print(batch)
        labels = batch.pop('labels')
        outputs = model(batch)
        
        loss = loss_fn(outputs, labels)
        loss.backward()
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def evaluate_epoch(model, loader, loss_fn, device, task, metric_name):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            #batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            labels = batch.pop('labels')
            outputs = model(batch)
            
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            
            if task == 'classification':
                preds = torch.argmax(outputs, dim=1)
            else: # regression
                preds = outputs
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    
    if task == 'classification':
        if metric_name == 'accuracy':
            metric = accuracy_score(all_labels, all_preds)
        elif metric_name == 'f1':
            metric = f1_score(all_labels, all_preds, average='weighted')
        else:
            raise ValueError(f"Metrica '{metric_name}' non supportata per la classificazione.")
    else: # regression
        metric = root_mean_squared_error(all_labels, all_preds) # RMSE
        
    return avg_loss, metric

class Trainer:
    def __init__(self, config, train_loader, val_loader, fold):
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.fold = fold
        self.device = torch.device(config.device)
        self.output_path = Path(config.output_dir) / f"fold_{fold}"
        self.output_path.mkdir(parents=True, exist_ok=True)

    def train(self):
        clear_memory()
        model = build_model(self.config).to(self.device)
        
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        
        num_training_steps = len(self.train_loader) * self.config.training.epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=400, num_training_steps=num_training_steps)
        
        loss_fn = nn.CrossEntropyLoss() if self.config.task == 'classification' else nn.MSELoss()

        best_metric = -np.inf if self.config.training.eval_metric in ['accuracy', 'f1'] else np.inf
        patience_counter = 0

        print(f"--- Inizio Training Fold {self.fold} ---")
        for epoch in range(self.config.training.epochs):
            train_loss = train_epoch(model, self.train_loader, optimizer, scheduler, loss_fn, self.device)
            val_loss, val_metric = evaluate_epoch(model, self.val_loader, loss_fn, self.device, self.config.task, self.config.training.eval_metric)
            
            print(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val {self.config.training.eval_metric}: {val_metric:.4f}")

            is_better = (val_metric > best_metric) if self.config.training.eval_metric in ['accuracy', 'f1'] else (val_metric < best_metric)
            
            if is_better:
                best_metric = val_metric
                torch.save(model.state_dict(), self.output_path / "best_model.pt")
                print(f"-> Modello salvato. Nuova metrica migliore: {best_metric:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config.training.early_stopping_patience:
                print("-> Early stopping attivato.")
                break
        print(f"--- Fine Training Fold {self.fold} ---")

class Predictor:
    def __init__(self, config, test_loader):
        self.config = config
        self.test_loader = test_loader
        self.device = torch.device(config.device)
        self.output_dir = Path(config.output_dir)
    
    def predict(self):
        all_fold_scores = []
        
        for fold in range(self.config.k_folds):
            print(f"--- Inferenza con Fold {fold} ---")
            model_path = self.output_dir / f"fold_{fold}" / "best_model.pt"
            if not model_path.exists():
                print(f"ATTENZIONE: Modello per il fold {fold} non trovato in {model_path}. Salto.")
                continue

            model = build_model(self.config)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.to(self.device)
            model.eval()
            
            fold_scores = []
            test_ids = []
            
            with torch.no_grad():
                for batch in tqdm(self.test_loader, desc=f"Predicting Fold {fold}", leave=False):
                    ids = batch.pop("id")
                    test_ids.extend(ids)
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    outputs = model(batch)
                    
                    if self.config.task == 'classification':
                        scores = torch.softmax(outputs, dim=1)[:, 1] # Probabilità della classe positiva
                    else: # regression
                        scores = outputs
                    
                    fold_scores.extend(scores.cpu().numpy())
            
            all_fold_scores.append(fold_scores)
            
        # Ensembling: media degli score/probabilità
        mean_scores = np.mean(all_fold_scores, axis=0)
        
        # Crea DataFrame finale
        results_df = pd.DataFrame({'ID': test_ids[:len(mean_scores)], 'score': mean_scores})
        
        if self.config.task == 'classification':
            results_df['prediction'] = (results_df['score'] >= 0.5).astype(int)
        else:
            results_df['prediction'] = results_df['score']
            
        output_file = self.output_dir / "predictions.csv"
        results_df[['ID', 'prediction']].to_csv(output_file, index=False)
        print(f"\nPredizioni salvate in {output_file}")
        
class Evaluator:
    def __init__(self, config):
        self.config = config
        self.predictions_path = Path(config.output_dir) / "predictions.csv"
        if config.task == 'classification':
            self.labels_path = Path(config.data.test_task1_labels)
        else:
            self.labels_path = Path(config.data.test_task2_labels)
    
    def evaluate(self):
        if not self.predictions_path.exists():
            raise FileNotFoundError(f"File di predizioni non trovato: {self.predictions_path}")
        
        preds_df = pd.read_csv(self.predictions_path)
        labels_df = pd.read_csv(self.labels_path)
        
        # Assicuriamoci che la colonna ID si chiami allo stesso modo per il merge
        # A volte i CSV hanno nomi leggermente diversi (es. 'id', 'ID', 'adressfname')
        if 'ID' not in labels_df.columns:
             # Cerca una colonna che potrebbe essere l'ID
             possible_id_cols = [col for col in labels_df.columns if 'id' in col.lower() or 'name' in col.lower()]
             if possible_id_cols:
                 labels_df = labels_df.rename(columns={possible_id_cols[0]: 'ID'})
             else:
                 raise ValueError("Non riesco a trovare la colonna ID nel file delle etichette.")

        merged_df = pd.merge(preds_df, labels_df, on="ID")
        
        y_pred = merged_df['prediction']
        
        if self.config.task == 'classification':
            # --- FIX CRUCIALE: MAPPING DELLE ETICHETTE ---
            # Convertiamo le stringhe 'Control'/'ProbableAD' in 0/1
            # Adatta questo dizionario se le tue etichette sono diverse (es. 'CN'/'AD')
            label_mapping = {'Control': 0, 'ProbableAD': 1, 'CN': 0, 'AD': 1}
            
            # Usa la colonna corretta per la diagnosi. Potrebbe chiamarsi 'Dx', 'diagnosis', ecc.
            # Cerchiamo di essere flessibili.
            dx_col = None
            for col in ['Dx', 'diagnosis', 'label']:
                if col in merged_df.columns:
                    dx_col = col
                    break
            
            if dx_col is None:
                 raise ValueError(f"Colonna diagnosi non trovata. Colonne disponibili: {merged_df.columns}")

            # Applica il mapping. Se un valore non è nel dizionario, potrebbe dare errore o NaN.
            try:
                y_true = merged_df[dx_col].map(label_mapping).astype(int)
            except ValueError as e:
                 print(f"Errore durante il mapping delle etichette. Valori unici trovati in {dx_col}: {merged_df[dx_col].unique()}")
                 raise e
            # ---------------------------------------------

            print("----- Valutazione Classificazione -----")
            print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
            print(f"F1-Score (Weighted): {f1_score(y_true, y_pred, average='weighted'):.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true, y_pred, target_names=['CN', 'AD']))
            print("\nConfusion Matrix:")
            print(confusion_matrix(y_true, y_pred))
            print("------------------------------------")
        else: # regression
            y_true = merged_df['MMSE']
            print("----- Valutazione Regressione -----")
            rmse = root_mean_squared_error(y_true, y_pred)
            print(f"RMSE: {rmse:.4f}")
            print("------------------------------------")
===== ./src/__init__.py =====

===== ./scripts/transcribe.py =====
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import torch
import gc

# Aggiunge la root del progetto al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config

# ===================================================================
#      IMPLEMENTAZIONE CORRETTA PER NYRAHEALTH/CRISPERWHISPER
# ===================================================================
def get_crisperwhisper_transcriber(config):
    """
    Prepara la pipeline per CrisperWhisper.
    Carica il token dal file .env solo per questo engine.
    """
    # --- MODIFICA CHIAVE: IMPORT E CARICAMENTO LOCALE ---
    # Questo codice viene eseguito SOLO se l'engine è "crisperwhisper".
    # In questo modo, gli altri ambienti non hanno bisogno di 'python-dotenv'.
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("Info: File .env trovato e caricato per l'autenticazione a Hugging Face.")
    except ImportError:
        print("Info: Libreria 'python-dotenv' non trovata. Si procederà usando il token salvato da 'huggingface-cli login' se disponibile.")
    # --- FINE MODIFICA ---

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    except ImportError:
        print("\nERRORE: La libreria 'transformers' (versione custom) non è installata.")
        print("Assicurati di aver attivato 'crisper_env' e di aver eseguito 'pip install git+...'")
        sys.exit(1)

    cfg = config.transcription.crisperwhisper
    device = config.device
    torch_dtype = torch.float16 if "cuda" in device else torch.float32
    
    print(f"Caricamento backend CrisperWhisper (HF Pipeline) con modello '{cfg.model_id}'...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        cfg.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(cfg.model_id)
    transcription_pipeline = pipeline(
        "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor, chunk_length_s=30,
        batch_size=cfg.batch_size, torch_dtype=torch_dtype, device=device,
    )
    print("Backend CrisperWhisper inizializzato.")

    def transcribe_files(audio_dir, output_dir):
        # ... (Questa logica interna rimane invariata)
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                result = transcription_pipeline(str(audio_path))
                full_text = result["text"].strip()
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files

# ===================================================================
#             LOGICA PER NEMO E WHISPERX (INVARIATE)
# ===================================================================

def get_nemo_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import nemo.collections.asr as nemo_asr
    except ImportError:
        print("\nERRORE: NVIDIA NeMo Toolkit non è installato. Attiva l'ambiente 'nemo_env'.")
        sys.exit(1)
    cfg = config.transcription.nemo
    print(f"Caricamento backend NeMo con modello '{cfg.model_name}'...")
    backend = nemo_asr.models.ASRModel.from_pretrained(model_name=cfg.model_name)
    backend.to(torch.device(config.device))
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o sono già tutti presenti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        str_paths = [str(p) for p in audio_files]
        result = backend.transcribe(paths_to_audio_files=str_paths, batch_size=cfg.batch_size, channel_selector='average', verbose=False)
        transcriptions = result[0] if isinstance(result, tuple) else result
        for audio_path, text_obj in zip(audio_files, transcriptions):
            final_text = text_obj.text if hasattr(text_obj, 'text') else text_obj
            out_path = output_dir / f"{audio_path.stem}.txt"
            out_path.write_text(final_text.strip() if final_text else "", encoding='utf-8')
    return transcribe_files


def get_whisperx_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import whisperx
    except ImportError:
        print("\nERRORE: La libreria 'whisperx' non è installata. Attiva l'ambiente 'whisperx_env'.")
        sys.exit(1)
    cfg = config.transcription.whisperx
    print(f"Caricamento backend WhisperX con modello '{cfg.model_name}'...")
    model = whisperx.load_model(cfg.model_name, config.device, compute_type=cfg.compute_type, language=cfg.language)
    print("Backend WhisperX inizializzato.")
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                audio = whisperx.load_audio(str(audio_path))
                result = model.transcribe(audio, batch_size=cfg.batch_size)
                full_text = " ".join([segment['text'].strip() for segment in result["segments"]])
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files


def main():
    # L'import di dotenv è stato rimosso da qui.
    parser = argparse.ArgumentParser(description="Genera trascrizioni per i dati.")
    # ... (il resto della funzione main rimane identico)
    parser.add_argument("--config", type=str, required=True, help="Percorso al file config.yaml")
    args = parser.parse_args()
    config = load_config(args.config)
    engine = config.transcription.engine.lower()
    transcribe_function = None
    if engine == "crisperwhisper":
        transcribe_function = get_crisperwhisper_transcriber(config)
    elif engine == "nemo":
        transcribe_function = get_nemo_transcriber(config)
    elif engine == "whisperx":
        transcribe_function = get_whisperx_transcriber(config)
    else:
        print(f"ERRORE: Engine '{engine}' non supportato. Scegli tra 'crisperwhisper', 'nemo' o 'whisperx'.")
        sys.exit(1)
    model_folder_name = ""
    if engine == "crisperwhisper":
        model_folder_name = config.transcription.crisperwhisper.model_id.split('/')[-1]
    elif engine == "nemo":
        model_folder_name = config.transcription.nemo.model_name.split('/')[-1]
    elif engine == "whisperx":
        model_folder_name = f"WhisperX_{config.transcription.whisperx.model_name}"
    train_output_dir = Path(config.data.transcripts_root) / model_folder_name
    test_output_dir = Path(config.data.test_transcripts_root) / model_folder_name
    train_output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Gli output verranno salvati in: {train_output_dir} e {test_output_dir}")
    print(f"\n--- Inizio Trascrizione Training Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.audio_root), train_output_dir)
    print(f"\n--- Inizio Trascrizione Test Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.test_audio_root), test_output_dir)
    print("\nTrascrizione completata.")

if __name__ == "__main__":
    main()
===== ./scripts/run.py =====
import argparse
import sys
from pathlib import Path

# Aggiunge la root del progetto al path per permettere 'from src import ...'
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.utils import set_seed
from src.data import get_data_splits, get_dataloaders
from src.engine import Trainer, Predictor, Evaluator

def main():
    parser = argparse.ArgumentParser(description="Script principale per training, predizione e valutazione.")
    parser.add_argument("--config", type=str, required=True, help="Percorso al file di configurazione (es. config.yaml)")
    parser.add_argument("--mode", type=str, required=True, choices=["train", "predict", "evaluate"], help="Modalità di esecuzione.")
    args = parser.parse_args()

    config = load_config(args.config)
    set_seed(config.seed)

    if args.mode == 'train':
        print(f"Inizio training per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Gli output verranno salvati in: {config.output_dir}")
        
        # Il K-Fold è gestito qui
        for fold, train_df, val_df, _ in get_data_splits(config):
            train_loader, val_loader = get_dataloaders(config, train_df, val_df)
            trainer = Trainer(config, train_loader, val_loader, fold)
            trainer.train()
            
    elif args.mode == 'predict':
        print(f"Inizio predizione per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Uso i modelli da: {config.output_dir}")
        
        # Ottieni il test loader (usiamo un ciclo fittizio per prendere il primo test_df)
        for _, _, _, test_df in get_data_splits(config):
            _, _, test_loader = get_dataloaders(config, train_df=test_df, val_df=test_df, test_df=test_df)
            break # Usciamo dopo il primo, dato che test_df è sempre lo stesso
            
        predictor = Predictor(config, test_loader)
        predictor.predict()

    elif args.mode == 'evaluate':
        print(f"Inizio valutazione per il task '{config.task}'.")
        print(f"Valuto le predizioni in: {Path(config.output_dir) / 'predictions.csv'}")
        
        evaluator = Evaluator(config)
        evaluator.evaluate()

if __name__ == "__main__":
    main()
===== ./config.yaml =====
# ===================================================================
#                    CONFIGURAZIONE PRINCIPALE
# ===================================================================

# --- Impostazioni Generali ---
seed: 42
device: "cuda"  # "cuda" o "cpu"

# --- Percorsi dei Dati ---
data:
  audio_root: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio"
  transcripts_root: "transcripts"
  train_labels: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv"
  
  test_audio_root: "Adresso21/ADReSSo21-diagnosis-test/ADReSSo21/diagnosis/test-dist/audio"
  test_transcripts_root: "transcripts_test"
  test_task1_labels: "Adresso21/label_test_task1.csv"
  test_task2_labels: "Adresso21/label_test_task2.csv"

# ===================================================================
#                   CONFIGURAZIONE TRASCRIZIONE (ASR)
# ===================================================================
# Modifica 'engine' per scegliere quale motore ASR usare.
# Lo script leggerà solo la sezione di configurazione corrispondente.
# ===================================================================

transcription:
  # CAMBIA QUI: "crisperwhisper", "nemo", o "whisperx"
  engine: "whisperx"
  overwrite: false

  # --- SEZIONE CRISPERWHISPER RIVISTA E CORRETTA ---
  crisperwhisper:
    # L'unico parametro che ci serve è l'ID del modello su Hugging Face
    model_id: "nyrahealth/CrisperWhisper" 
    batch_size: 16
    compute_type: "float16" # o "int8"
    
  # --- Impostazioni per NeMo ---
  nemo:
    model_name: "nvidia/parakeet-tdt-0.6b-v2"
    batch_size: 16

  # --- Impostazioni per WhisperX ---
  whisperx:
    model_name: "nyrahealth/faster_CrisperWhisper"
    batch_size: 16
    compute_type: "float16"
    language: "en"
# ===================================================================
#                     CONFIGURAZIONE ESPERIMENTO
# ===================================================================
# Questa sezione controlla le operazioni di training, predizione e valutazione
# lanciate con 'python scripts/run.py'.
# ===================================================================

# --- Task e Modello ---
task: "classification"    # "classification" o "regression"
modality: "audio"          # "text" o "audio"

# Modello di trascrizione da usare per l'addestramento del modello testuale.
# Deve corrispondere al nome della cartella creata da 'transcribe.py'.
# Esempio per Crisper: "Crisper_whisper-large-v3"
# Esempio per NeMo: "parakeet-tdt-0.6b-v2"
transcription_model_for_training: "large-v3"

# --- Cross-Validation ---
k_folds: 5
output_dir: "outputs/bert_classification_parakeet" # Cartella dove salvare modelli e predizioni

# --- Parametri del Modello ---
model:
  text:
    name: "bert-large-uncased"
    max_length: 275
    dropout: 0.1

  audio:
    name: "ecapa-tdnn"
    pretrained: "speechbrain/spkrec-ecapa-voxceleb"
    sample_rate: 16000
    trainable_encoder: true 
    dropout: 0.1

# --- Parametri di Addestramento ---
training:
  epochs: 15
  batch_size: 1
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 400
  early_stopping_patience: 3
  eval_metric: "accuracy" # Per classificazione: "accuracy" o "f1". Per regressione: "rmse".
===== ./src/models.py =====
import torch
import torch.nn as nn
from transformers import AutoModel
from speechbrain.pretrained import EncoderClassifier
import sys

class BertClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = AutoModel.from_pretrained(config.model.text.name)
        self.dropout = nn.Dropout(config.model.text.dropout)
        num_classes = 2 if config.task == 'classification' else 1
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
        nn.init.xavier_uniform_(self.classifier.weight)
        if self.classifier.bias is not None:
            nn.init.zeros_(self.classifier.bias)
            
    def forward(self, batch):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        if self.classifier.out_features > 1:
            return logits
        else:
            return logits.squeeze(-1)

class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = EncoderClassifier.from_hparams(source=config.model.audio.pretrained)
        
        if not config.model.audio.trainable_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = False

        last_layer = self.encoder.hparams.embedding_model.fc
        num_features = last_layer.conv.out_channels
        
        num_classes = 2 if config.task == 'classification' else 1
        
        self.classifier_head = nn.Sequential(
            nn.Linear(num_features, num_features // 2),
            nn.ReLU(),
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features // 2, num_classes)
        )

        for module in self.classifier_head.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

# ./src/models.py
class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = EncoderClassifier.from_hparams(source=config.model.audio.pretrained)

        if not config.model.audio.trainable_encoder:
            for p in self.encoder.parameters():
                p.requires_grad = False

        # dimensione embedding in modo robusto (senza introspezione fragile)
        with torch.no_grad():
            dummy = torch.zeros(1, config.model.audio.sample_rate)  # ~1s a 16kHz
            emb = self.encoder.encode_batch(dummy)                  # [1, 1, D] o [1, D]
            emb = emb.squeeze(1) if emb.dim() == 3 and emb.size(1) == 1 else emb
            num_features = emb.shape[-1]

        num_classes = 2 if config.task == 'classification' else 1
        self.classifier_head = nn.Sequential(
            nn.Linear(num_features, num_features // 2),
            nn.ReLU(),
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features // 2, num_classes),
        )
        for m in self.classifier_head.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, batch):
        wavs = batch['waveform']                       # [B, T] (DA NON UNSQUEEZE!)
        wav_lens = batch.get('lengths', None)

        device = next(self.parameters()).device
        wavs = wavs.to(device)
        if wav_lens is not None:
            wav_lens = wav_lens.to(device)

        # Se l'encoder è congelato, evitiamo di tracciare i grad; altrimenti abilitiamo
        grad_enabled = any(p.requires_grad for p in self.encoder.parameters())
        with torch.set_grad_enabled(grad_enabled):
            emb = self.encoder.encode_batch(wavs, wav_lens=wav_lens)  # [B,1,D] o [B,D]

        if emb.dim() == 3 and emb.size(1) == 1:
            emb = emb.squeeze(1)  # -> [B, D]

        logits = self.classifier_head(emb)
        return logits if self.classifier_head[-1].out_features > 1 else logits.squeeze(-1)


def build_model(config):
    if config.modality == 'text':
        return BertClassifier(config)
    elif config.modality == 'audio':
        return EcapaTdnnClassifier(config)
    else:
        raise ValueError(f"Modello per la modalità '{config.modality}' non supportato.")
===== ./src/data.py =====
from pathlib import Path
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from sklearn.model_selection import StratifiedKFold
import torchaudio
from torchaudio.functional import resample

class TextDataset(Dataset):
    # --- QUESTA CLASSE È CORRETTA E RIMANE INVARIATA ---
    def __init__(self, df: pd.DataFrame, config, tokenizer):
        self.df = df; self.config = config; self.tokenizer = tokenizer; self.is_test = 'label' not in df.columns
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        transcript_dir = Path(self.config.data.transcripts_root if not self.is_test else self.config.data.test_transcripts_root)
        transcript_path = transcript_dir / self.config.transcription_model_for_training / f"{file_id}.txt"
        try: text = transcript_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError: text = ""
        inputs = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.config.model.text.max_length,
            padding="max_length", truncation=True, return_attention_mask=True, return_tensors="pt",
        )
        item = {"input_ids": inputs["input_ids"].flatten(), "attention_mask": inputs["attention_mask"].flatten(), "id": file_id}
        if not self.is_test:
            item['labels'] = torch.tensor(row['label'], dtype=torch.long if self.config.task == 'classification' else torch.float32)
        return item

class AudioDataset(Dataset):
    def __init__(self, df: pd.DataFrame, config):
        self.df = df
        self.config = config
        self.is_test = 'label' not in df.columns
        self.audio_root = Path(self.config.data.audio_root if not self.is_test else self.config.data.test_audio_root)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        audio_path = self.audio_root / f"{file_id}.wav"
        if not audio_path.exists() and not self.is_test:
            group = 'ad' if row['label'] == 1 else 'cn'
            audio_path = self.audio_root / group / f"{file_id}.wav"

        waveform, sr = torchaudio.load(audio_path)
        
        # --- FIX #1: FORZARE L'AUDIO MONO ---
        # Se la waveform ha più di un canale (es. è stereo), ne facciamo la media
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        # ------------------------------------

        if sr != self.config.model.audio.sample_rate:
            waveform = resample(waveform, sr, self.config.model.audio.sample_rate)
        
        # Ora .squeeze() funzionerà sempre, perché l'input è sempre [1, n_samples]
        item = {"waveform": waveform.squeeze(0), "id": file_id}

        if not self.is_test:
            target = row['label']
            dtype = torch.long if self.config.task == 'classification' else torch.float32
            item['labels'] = torch.tensor(target, dtype=dtype)
            
        return item

def collate_audio(batch):
    waveforms = [item['waveform'] for item in batch]
    ids = [item['id'] for item in batch]

    # lunghezze "reali" prima del padding
    lengths = [w.shape[-1] for w in waveforms]
    padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)

    # SpeechBrain si aspetta frazioni (0..1) rispetto alla max len del batch
    max_len = padded_waveforms.shape[1]
    wav_lens = torch.tensor([L / max_len for L in lengths], dtype=torch.float32)

    collated_batch = {"waveform": padded_waveforms, "lengths": wav_lens, "id": ids}
    if 'labels' in batch[0]:
        labels = torch.stack([item['labels'] for item in batch])
        collated_batch['labels'] = labels
    return collated_batch


def get_data_splits(config):
    """Prepara i DataFrame per training, validazione e test."""
    train_df = pd.read_csv(config.data.train_labels)
    train_df = train_df.rename(columns={'adressfname': 'ID', 'dx': 'diagnosis'})
    train_df = train_df.set_index('ID')
    
    if config.task == 'classification':
        train_df['label'] = train_df['diagnosis'].apply(lambda x: 1 if x == 'ad' else 0)
    else:
        train_df['label'] = train_df['mmse']
        
    # --- FIX #2: RIMOSSO .sort_index() ERRATO ---
    # Mescoliamo il dataframe una volta per sicurezza, senza riordinarlo.
    train_df = train_df.sample(frac=1, random_state=config.seed)
    # ---------------------------------------------
        
    test_df = pd.read_csv(config.data.test_task1_labels).rename(columns={'ID': 'id'}).set_index('id')

    skf = StratifiedKFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):
        train_split = train_df.iloc[train_idx]
        val_split = train_df.iloc[val_idx]
        yield fold, train_split, val_split, test_df
        
def get_dataloaders(config, train_df, val_df, test_df=None):
    if config.modality == 'text':
        tokenizer = AutoTokenizer.from_pretrained(config.model.text.name)
        train_dataset = TextDataset(train_df, config, tokenizer)
        val_dataset = TextDataset(val_df, config, tokenizer)
        collate_fn = None
    elif config.modality == 'audio':
        train_dataset = AudioDataset(train_df, config)
        val_dataset = AudioDataset(val_df, config)
        collate_fn = collate_audio
    else:
        raise ValueError(f"Modalità '{config.modality}' non supportata.")

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
    
    if test_df is not None:
        test_dataset = TextDataset(test_df, config, tokenizer) if config.modality == 'text' else AudioDataset(test_df, config)
        test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader, test_loader
        
    return train_loader, val_loader
===== ./src/config.py =====
import yaml
from pathlib import Path
from typing import Any

class Config:
    """
    Classe per caricare la configurazione da un file YAML e permettere
    l'accesso agli attributi tramite dot notation (es. config.data.audio_root).
    """
    def __init__(self, data: dict):
        for key, value in data.items():
            if isinstance(value, dict):
                setattr(self, key, Config(value))
            else:
                setattr(self, key, value)

    def __repr__(self):
        return str(self.__dict__)

    def to_dict(self) -> dict:
        result = {}
        for key, value in self.__dict__.items():
            if isinstance(value, Config):
                result[key] = value.to_dict()
            else:
                result[key] = value
        return result

def load_config(config_path: str | Path) -> Config:
    """Carica un file di configurazione YAML."""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"File di configurazione non trovato in: {path.resolve()}")
        
    with open(path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if not isinstance(config_data, dict):
        raise TypeError("La radice del file YAML deve essere un dizionario.")
        
    return Config(config_data)
===== ./src/utils.py =====
import torch
import numpy as np
import random
import gc

def set_seed(seed: int):
    """Imposta il seed per la riproducibilità."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def clear_memory():
    """Libera la memoria della GPU."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
===== ./src/engine.py =====
import torch
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, classification_report, confusion_matrix

from src.models import build_model
from src.utils import clear_memory

def train_epoch(model, loader, optimizer, scheduler, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in tqdm(loader, desc="Training", leave=False):
        optimizer.zero_grad()
        
        # Sposta il batch sul device corretto
        #batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        #print(batch)
        labels = batch.pop('labels')
        outputs = model(batch)
        
        loss = loss_fn(outputs, labels)
        loss.backward()
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def evaluate_epoch(model, loader, loss_fn, device, task, metric_name):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            #batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            labels = batch.pop('labels')
            outputs = model(batch)
            
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            
            if task == 'classification':
                preds = torch.argmax(outputs, dim=1)
            else: # regression
                preds = outputs
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    
    if task == 'classification':
        if metric_name == 'accuracy':
            metric = accuracy_score(all_labels, all_preds)
        elif metric_name == 'f1':
            metric = f1_score(all_labels, all_preds, average='weighted')
        else:
            raise ValueError(f"Metrica '{metric_name}' non supportata per la classificazione.")
    else: # regression
        metric = root_mean_squared_error(all_labels, all_preds) # RMSE
        
    return avg_loss, metric

class Trainer:
    def __init__(self, config, train_loader, val_loader, fold):
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.fold = fold
        self.device = torch.device(config.device)
        self.output_path = Path(config.output_dir) / f"fold_{fold}"
        self.output_path.mkdir(parents=True, exist_ok=True)

    def train(self):
        clear_memory()
        model = build_model(self.config).to(self.device)
        
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        
        num_training_steps = len(self.train_loader) * self.config.training.epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=400, num_training_steps=num_training_steps)
        
        loss_fn = nn.CrossEntropyLoss() if self.config.task == 'classification' else nn.MSELoss()

        best_metric = -np.inf if self.config.training.eval_metric in ['accuracy', 'f1'] else np.inf
        patience_counter = 0

        print(f"--- Inizio Training Fold {self.fold} ---")
        for epoch in range(self.config.training.epochs):
            train_loss = train_epoch(model, self.train_loader, optimizer, scheduler, loss_fn, self.device)
            val_loss, val_metric = evaluate_epoch(model, self.val_loader, loss_fn, self.device, self.config.task, self.config.training.eval_metric)
            
            print(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val {self.config.training.eval_metric}: {val_metric:.4f}")

            is_better = (val_metric > best_metric) if self.config.training.eval_metric in ['accuracy', 'f1'] else (val_metric < best_metric)
            
            if is_better:
                best_metric = val_metric
                torch.save(model.state_dict(), self.output_path / "best_model.pt")
                print(f"-> Modello salvato. Nuova metrica migliore: {best_metric:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config.training.early_stopping_patience:
                print("-> Early stopping attivato.")
                break
        print(f"--- Fine Training Fold {self.fold} ---")

class Predictor:
    def __init__(self, config, test_loader):
        self.config = config
        self.test_loader = test_loader
        self.device = torch.device(config.device)
        self.output_dir = Path(config.output_dir)
    
    def predict(self):
        all_fold_scores = []
        
        for fold in range(self.config.k_folds):
            print(f"--- Inferenza con Fold {fold} ---")
            model_path = self.output_dir / f"fold_{fold}" / "best_model.pt"
            if not model_path.exists():
                print(f"ATTENZIONE: Modello per il fold {fold} non trovato in {model_path}. Salto.")
                continue

            model = build_model(self.config)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.to(self.device)
            model.eval()
            
            fold_scores = []
            test_ids = []
            
            with torch.no_grad():
                for batch in tqdm(self.test_loader, desc=f"Predicting Fold {fold}", leave=False):
                    ids = batch.pop("id")
                    test_ids.extend(ids)
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    outputs = model(batch)
                    
                    if self.config.task == 'classification':
                        scores = torch.softmax(outputs, dim=1)[:, 1] # Probabilità della classe positiva
                    else: # regression
                        scores = outputs
                    
                    fold_scores.extend(scores.cpu().numpy())
            
            all_fold_scores.append(fold_scores)
            
        # Ensembling: media degli score/probabilità
        mean_scores = np.mean(all_fold_scores, axis=0)
        
        # Crea DataFrame finale
        results_df = pd.DataFrame({'ID': test_ids[:len(mean_scores)], 'score': mean_scores})
        
        if self.config.task == 'classification':
            results_df['prediction'] = (results_df['score'] >= 0.5).astype(int)
        else:
            results_df['prediction'] = results_df['score']
            
        output_file = self.output_dir / "predictions.csv"
        results_df[['ID', 'prediction']].to_csv(output_file, index=False)
        print(f"\nPredizioni salvate in {output_file}")
        
class Evaluator:
    def __init__(self, config):
        self.config = config
        self.predictions_path = Path(config.output_dir) / "predictions.csv"
        if config.task == 'classification':
            self.labels_path = Path(config.data.test_task1_labels)
        else:
            self.labels_path = Path(config.data.test_task2_labels)
    
    def evaluate(self):
        if not self.predictions_path.exists():
            raise FileNotFoundError(f"File di predizioni non trovato: {self.predictions_path}")
        
        preds_df = pd.read_csv(self.predictions_path)
        labels_df = pd.read_csv(self.labels_path)
        
        # Assicuriamoci che la colonna ID si chiami allo stesso modo per il merge
        # A volte i CSV hanno nomi leggermente diversi (es. 'id', 'ID', 'adressfname')
        if 'ID' not in labels_df.columns:
             # Cerca una colonna che potrebbe essere l'ID
             possible_id_cols = [col for col in labels_df.columns if 'id' in col.lower() or 'name' in col.lower()]
             if possible_id_cols:
                 labels_df = labels_df.rename(columns={possible_id_cols[0]: 'ID'})
             else:
                 raise ValueError("Non riesco a trovare la colonna ID nel file delle etichette.")

        merged_df = pd.merge(preds_df, labels_df, on="ID")
        
        y_pred = merged_df['prediction']
        
        if self.config.task == 'classification':
            # --- FIX CRUCIALE: MAPPING DELLE ETICHETTE ---
            # Convertiamo le stringhe 'Control'/'ProbableAD' in 0/1
            # Adatta questo dizionario se le tue etichette sono diverse (es. 'CN'/'AD')
            label_mapping = {'Control': 0, 'ProbableAD': 1, 'CN': 0, 'AD': 1}
            
            # Usa la colonna corretta per la diagnosi. Potrebbe chiamarsi 'Dx', 'diagnosis', ecc.
            # Cerchiamo di essere flessibili.
            dx_col = None
            for col in ['Dx', 'diagnosis', 'label']:
                if col in merged_df.columns:
                    dx_col = col
                    break
            
            if dx_col is None:
                 raise ValueError(f"Colonna diagnosi non trovata. Colonne disponibili: {merged_df.columns}")

            # Applica il mapping. Se un valore non è nel dizionario, potrebbe dare errore o NaN.
            try:
                y_true = merged_df[dx_col].map(label_mapping).astype(int)
            except ValueError as e:
                 print(f"Errore durante il mapping delle etichette. Valori unici trovati in {dx_col}: {merged_df[dx_col].unique()}")
                 raise e
            # ---------------------------------------------

            print("----- Valutazione Classificazione -----")
            print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
            print(f"F1-Score (Weighted): {f1_score(y_true, y_pred, average='weighted'):.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true, y_pred, target_names=['CN', 'AD']))
            print("\nConfusion Matrix:")
            print(confusion_matrix(y_true, y_pred))
            print("------------------------------------")
        else: # regression
            y_true = merged_df['MMSE']
            print("----- Valutazione Regressione -----")
            rmse = root_mean_squared_error(y_true, y_pred)
            print(f"RMSE: {rmse:.4f}")
            print("------------------------------------")
===== ./src/__init__.py =====

===== ./scripts/transcribe.py =====
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import torch
import gc

# Aggiunge la root del progetto al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config

# ===================================================================
#      IMPLEMENTAZIONE CORRETTA PER NYRAHEALTH/CRISPERWHISPER
# ===================================================================
def get_crisperwhisper_transcriber(config):
    """
    Prepara la pipeline per CrisperWhisper.
    Carica il token dal file .env solo per questo engine.
    """
    # --- MODIFICA CHIAVE: IMPORT E CARICAMENTO LOCALE ---
    # Questo codice viene eseguito SOLO se l'engine è "crisperwhisper".
    # In questo modo, gli altri ambienti non hanno bisogno di 'python-dotenv'.
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("Info: File .env trovato e caricato per l'autenticazione a Hugging Face.")
    except ImportError:
        print("Info: Libreria 'python-dotenv' non trovata. Si procederà usando il token salvato da 'huggingface-cli login' se disponibile.")
    # --- FINE MODIFICA ---

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    except ImportError:
        print("\nERRORE: La libreria 'transformers' (versione custom) non è installata.")
        print("Assicurati di aver attivato 'crisper_env' e di aver eseguito 'pip install git+...'")
        sys.exit(1)

    cfg = config.transcription.crisperwhisper
    device = config.device
    torch_dtype = torch.float16 if "cuda" in device else torch.float32
    
    print(f"Caricamento backend CrisperWhisper (HF Pipeline) con modello '{cfg.model_id}'...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        cfg.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(cfg.model_id)
    transcription_pipeline = pipeline(
        "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor, chunk_length_s=30,
        batch_size=cfg.batch_size, torch_dtype=torch_dtype, device=device,
    )
    print("Backend CrisperWhisper inizializzato.")

    def transcribe_files(audio_dir, output_dir):
        # ... (Questa logica interna rimane invariata)
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                result = transcription_pipeline(str(audio_path))
                full_text = result["text"].strip()
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files

# ===================================================================
#             LOGICA PER NEMO E WHISPERX (INVARIATE)
# ===================================================================

def get_nemo_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import nemo.collections.asr as nemo_asr
    except ImportError:
        print("\nERRORE: NVIDIA NeMo Toolkit non è installato. Attiva l'ambiente 'nemo_env'.")
        sys.exit(1)
    cfg = config.transcription.nemo
    print(f"Caricamento backend NeMo con modello '{cfg.model_name}'...")
    backend = nemo_asr.models.ASRModel.from_pretrained(model_name=cfg.model_name)
    backend.to(torch.device(config.device))
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o sono già tutti presenti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        str_paths = [str(p) for p in audio_files]
        result = backend.transcribe(paths_to_audio_files=str_paths, batch_size=cfg.batch_size, channel_selector='average', verbose=False)
        transcriptions = result[0] if isinstance(result, tuple) else result
        for audio_path, text_obj in zip(audio_files, transcriptions):
            final_text = text_obj.text if hasattr(text_obj, 'text') else text_obj
            out_path = output_dir / f"{audio_path.stem}.txt"
            out_path.write_text(final_text.strip() if final_text else "", encoding='utf-8')
    return transcribe_files


def get_whisperx_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import whisperx
    except ImportError:
        print("\nERRORE: La libreria 'whisperx' non è installata. Attiva l'ambiente 'whisperx_env'.")
        sys.exit(1)
    cfg = config.transcription.whisperx
    print(f"Caricamento backend WhisperX con modello '{cfg.model_name}'...")
    model = whisperx.load_model(cfg.model_name, config.device, compute_type=cfg.compute_type, language=cfg.language)
    print("Backend WhisperX inizializzato.")
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                audio = whisperx.load_audio(str(audio_path))
                result = model.transcribe(audio, batch_size=cfg.batch_size)
                full_text = " ".join([segment['text'].strip() for segment in result["segments"]])
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files


def main():
    # L'import di dotenv è stato rimosso da qui.
    parser = argparse.ArgumentParser(description="Genera trascrizioni per i dati.")
    # ... (il resto della funzione main rimane identico)
    parser.add_argument("--config", type=str, required=True, help="Percorso al file config.yaml")
    args = parser.parse_args()
    config = load_config(args.config)
    engine = config.transcription.engine.lower()
    transcribe_function = None
    if engine == "crisperwhisper":
        transcribe_function = get_crisperwhisper_transcriber(config)
    elif engine == "nemo":
        transcribe_function = get_nemo_transcriber(config)
    elif engine == "whisperx":
        transcribe_function = get_whisperx_transcriber(config)
    else:
        print(f"ERRORE: Engine '{engine}' non supportato. Scegli tra 'crisperwhisper', 'nemo' o 'whisperx'.")
        sys.exit(1)
    model_folder_name = ""
    if engine == "crisperwhisper":
        model_folder_name = config.transcription.crisperwhisper.model_id.split('/')[-1]
    elif engine == "nemo":
        model_folder_name = config.transcription.nemo.model_name.split('/')[-1]
    elif engine == "whisperx":
        model_folder_name = f"WhisperX_{config.transcription.whisperx.model_name}"
    train_output_dir = Path(config.data.transcripts_root) / model_folder_name
    test_output_dir = Path(config.data.test_transcripts_root) / model_folder_name
    train_output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Gli output verranno salvati in: {train_output_dir} e {test_output_dir}")
    print(f"\n--- Inizio Trascrizione Training Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.audio_root), train_output_dir)
    print(f"\n--- Inizio Trascrizione Test Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.test_audio_root), test_output_dir)
    print("\nTrascrizione completata.")

if __name__ == "__main__":
    main()
===== ./scripts/run.py =====
import argparse
import sys
from pathlib import Path

# Aggiunge la root del progetto al path per permettere 'from src import ...'
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.utils import set_seed
from src.data import get_data_splits, get_dataloaders
from src.engine import Trainer, Predictor, Evaluator

def main():
    parser = argparse.ArgumentParser(description="Script principale per training, predizione e valutazione.")
    parser.add_argument("--config", type=str, required=True, help="Percorso al file di configurazione (es. config.yaml)")
    parser.add_argument("--mode", type=str, required=True, choices=["train", "predict", "evaluate"], help="Modalità di esecuzione.")
    args = parser.parse_args()

    config = load_config(args.config)
    set_seed(config.seed)

    if args.mode == 'train':
        print(f"Inizio training per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Gli output verranno salvati in: {config.output_dir}")
        
        # Il K-Fold è gestito qui
        for fold, train_df, val_df, _ in get_data_splits(config):
            train_loader, val_loader = get_dataloaders(config, train_df, val_df)
            trainer = Trainer(config, train_loader, val_loader, fold)
            trainer.train()
            
    elif args.mode == 'predict':
        print(f"Inizio predizione per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Uso i modelli da: {config.output_dir}")
        
        # Ottieni il test loader (usiamo un ciclo fittizio per prendere il primo test_df)
        for _, _, _, test_df in get_data_splits(config):
            _, _, test_loader = get_dataloaders(config, train_df=test_df, val_df=test_df, test_df=test_df)
            break # Usciamo dopo il primo, dato che test_df è sempre lo stesso
            
        predictor = Predictor(config, test_loader)
        predictor.predict()

    elif args.mode == 'evaluate':
        print(f"Inizio valutazione per il task '{config.task}'.")
        print(f"Valuto le predizioni in: {Path(config.output_dir) / 'predictions.csv'}")
        
        evaluator = Evaluator(config)
        evaluator.evaluate()

if __name__ == "__main__":
    main()
===== ./config.yaml =====
# ===================================================================
#                    CONFIGURAZIONE PRINCIPALE
# ===================================================================

# --- Impostazioni Generali ---
seed: 42
device: "cuda"  # "cuda" o "cpu"

# --- Percorsi dei Dati ---
data:
  audio_root: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio"
  transcripts_root: "transcripts"
  train_labels: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv"
  
  test_audio_root: "Adresso21/ADReSSo21-diagnosis-test/ADReSSo21/diagnosis/test-dist/audio"
  test_transcripts_root: "transcripts_test"
  test_task1_labels: "Adresso21/label_test_task1.csv"
  test_task2_labels: "Adresso21/label_test_task2.csv"

# ===================================================================
#                   CONFIGURAZIONE TRASCRIZIONE (ASR)
# ===================================================================
# Modifica 'engine' per scegliere quale motore ASR usare.
# Lo script leggerà solo la sezione di configurazione corrispondente.
# ===================================================================

transcription:
  # CAMBIA QUI: "crisperwhisper", "nemo", o "whisperx"
  engine: "whisperx"
  overwrite: false

  # --- SEZIONE CRISPERWHISPER RIVISTA E CORRETTA ---
  crisperwhisper:
    # L'unico parametro che ci serve è l'ID del modello su Hugging Face
    model_id: "nyrahealth/CrisperWhisper" 
    batch_size: 16
    compute_type: "float16" # o "int8"
    
  # --- Impostazioni per NeMo ---
  nemo:
    model_name: "nvidia/parakeet-tdt-0.6b-v2"
    batch_size: 16

  # --- Impostazioni per WhisperX ---
  whisperx:
    model_name: "nyrahealth/faster_CrisperWhisper"
    batch_size: 16
    compute_type: "float16"
    language: "en"
# ===================================================================
#                     CONFIGURAZIONE ESPERIMENTO
# ===================================================================
# Questa sezione controlla le operazioni di training, predizione e valutazione
# lanciate con 'python scripts/run.py'.
# ===================================================================

# --- Task e Modello ---
task: "classification"    # "classification" o "regression"
modality: "audio"          # "text" o "audio"

# Modello di trascrizione da usare per l'addestramento del modello testuale.
# Deve corrispondere al nome della cartella creata da 'transcribe.py'.
# Esempio per Crisper: "Crisper_whisper-large-v3"
# Esempio per NeMo: "parakeet-tdt-0.6b-v2"
transcription_model_for_training: "large-v3"

# --- Cross-Validation ---
k_folds: 5
output_dir: "outputs/bert_classification_parakeet" # Cartella dove salvare modelli e predizioni

# --- Parametri del Modello ---
model:
  text:
    name: "bert-large-uncased"
    max_length: 275
    dropout: 0.1

  audio:
    name: "ecapa-tdnn"
    pretrained: "speechbrain/spkrec-ecapa-voxceleb"
    sample_rate: 16000
    trainable_encoder: true 
    dropout: 0.1

# --- Parametri di Addestramento ---
training:
  epochs: 15
  batch_size: 1
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 400
  early_stopping_patience: 3
  eval_metric: "accuracy" # Per classificazione: "accuracy" o "f1". Per regressione: "rmse".
===== ./src/models.py =====
import torch
import torch.nn as nn
from transformers import AutoModel
from speechbrain.pretrained import EncoderClassifier
import sys

class BertClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = AutoModel.from_pretrained(config.model.text.name)
        self.dropout = nn.Dropout(config.model.text.dropout)
        num_classes = 2 if config.task == 'classification' else 1
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
        nn.init.xavier_uniform_(self.classifier.weight)
        if self.classifier.bias is not None:
            nn.init.zeros_(self.classifier.bias)
            
    def forward(self, batch):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        if self.classifier.out_features > 1:
            return logits
        else:
            return logits.squeeze(-1)

class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = EncoderClassifier.from_hparams(source=config.model.audio.pretrained)
        
        # Congeliamo l'encoder se richiesto
        if not config.model.audio.trainable_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = False

        # --- MODO ROBUSTO PER CAPIRE LA DIMENSIONE DELL'EMBEDDING ---
        # Creiamo un finto input e vediamo la dimensione dell'output,
        # così non dobbiamo fare affidamento sulla struttura interna del modello.
        with torch.no_grad():
            dummy_input = torch.zeros(1, config.model.audio.sample_rate) # 1 secondo di audio finto
            dummy_embedding = self.encoder.encode_batch(dummy_input)
            num_features = dummy_embedding.shape[-1]
        # -------------------------------------------------------------
        
        num_classes = 2 if config.task == 'classification' else 1
        
        self.classifier_head = nn.Sequential(
            nn.Linear(num_features, num_features // 2),
            nn.ReLU(),
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features // 2, num_classes)
        )

        for module in self.classifier_head.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, batch):
        # Prendiamo waveforms e lengths dal batch (devono essere già sul device giusto)
        waveforms = batch['waveform'] 
        
        # La funzione encode_batch di Speechbrain è il modo corretto per ottenere gli embedding.
        # Gestisce da sola feature, padding e tutto il resto.
        # Usiamo il gradiente solo se l'encoder è addestrabile
        is_trainable = any(p.requires_grad for p in self.encoder.parameters())
        with torch.set_grad_enabled(is_trainable):
            embeddings = self.encoder.encode_batch(waveforms) # Output: [Batch, 1, EmbeddingSize]

        # Rimuoviamo la dimensione extra
        embeddings = embeddings.squeeze(1) 
        
        outputs = self.classifier_head(embeddings)
        
        if self.classifier_head[-1].out_features > 1:
            return outputs
        else:
            return outputs.squeeze(-1)

def build_model(config):
    if config.modality == 'text':
        return BertClassifier(config)
    elif config.modality == 'audio':
        return EcapaTdnnClassifier(config)
    else:
        raise ValueError(f"Modello per la modalità '{config.modality}' non supportato.")
===== ./src/data.py =====
from pathlib import Path
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from sklearn.model_selection import StratifiedKFold
import torchaudio
from torchaudio.functional import resample

class TextDataset(Dataset):
    # --- QUESTA CLASSE È CORRETTA E RIMANE INVARIATA ---
    def __init__(self, df: pd.DataFrame, config, tokenizer):
        self.df = df; self.config = config; self.tokenizer = tokenizer; self.is_test = 'label' not in df.columns
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        transcript_dir = Path(self.config.data.transcripts_root if not self.is_test else self.config.data.test_transcripts_root)
        transcript_path = transcript_dir / self.config.transcription_model_for_training / f"{file_id}.txt"
        try: text = transcript_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError: text = ""
        inputs = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.config.model.text.max_length,
            padding="max_length", truncation=True, return_attention_mask=True, return_tensors="pt",
        )
        item = {"input_ids": inputs["input_ids"].flatten(), "attention_mask": inputs["attention_mask"].flatten(), "id": file_id}
        if not self.is_test:
            item['labels'] = torch.tensor(row['label'], dtype=torch.long if self.config.task == 'classification' else torch.float32)
        return item

class AudioDataset(Dataset):
    def __init__(self, df: pd.DataFrame, config):
        self.df = df
        self.config = config
        self.is_test = 'label' not in df.columns
        self.audio_root = Path(self.config.data.audio_root if not self.is_test else self.config.data.test_audio_root)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        audio_path = self.audio_root / f"{file_id}.wav"
        if not audio_path.exists() and not self.is_test:
            group = 'ad' if row['label'] == 1 else 'cn'
            audio_path = self.audio_root / group / f"{file_id}.wav"

        waveform, sr = torchaudio.load(audio_path)
        
        # --- FIX #1: FORZARE L'AUDIO MONO ---
        # Se la waveform ha più di un canale (es. è stereo), ne facciamo la media
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        # ------------------------------------

        if sr != self.config.model.audio.sample_rate:
            waveform = resample(waveform, sr, self.config.model.audio.sample_rate)
        
        # Ora .squeeze() funzionerà sempre, perché l'input è sempre [1, n_samples]
        item = {"waveform": waveform.squeeze(0), "id": file_id}

        if not self.is_test:
            target = row['label']
            dtype = torch.long if self.config.task == 'classification' else torch.float32
            item['labels'] = torch.tensor(target, dtype=dtype)
            
        return item

def collate_audio(batch):
    waveforms = [item['waveform'] for item in batch]
    ids = [item['id'] for item in batch]
    padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)
    collated_batch = {"waveform": padded_waveforms, "id": ids}
    if 'labels' in batch[0]:
        labels = torch.stack([item['labels'] for item in batch])
        collated_batch['labels'] = labels
    return collated_batch


def get_data_splits(config):
    """Prepara i DataFrame per training, validazione e test."""
    train_df = pd.read_csv(config.data.train_labels)
    train_df = train_df.rename(columns={'adressfname': 'ID', 'dx': 'diagnosis'})
    train_df = train_df.set_index('ID')
    
    if config.task == 'classification':
        train_df['label'] = train_df['diagnosis'].apply(lambda x: 1 if x == 'ad' else 0)
    else:
        train_df['label'] = train_df['mmse']
        
    # --- FIX #2: RIMOSSO .sort_index() ERRATO ---
    # Mescoliamo il dataframe una volta per sicurezza, senza riordinarlo.
    train_df = train_df.sample(frac=1, random_state=config.seed)
    # ---------------------------------------------
        
    test_df = pd.read_csv(config.data.test_task1_labels).rename(columns={'ID': 'id'}).set_index('id')

    skf = StratifiedKFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):
        train_split = train_df.iloc[train_idx]
        val_split = train_df.iloc[val_idx]
        yield fold, train_split, val_split, test_df
        
def get_dataloaders(config, train_df, val_df, test_df=None):
    if config.modality == 'text':
        tokenizer = AutoTokenizer.from_pretrained(config.model.text.name)
        train_dataset = TextDataset(train_df, config, tokenizer)
        val_dataset = TextDataset(val_df, config, tokenizer)
        collate_fn = None
    elif config.modality == 'audio':
        train_dataset = AudioDataset(train_df, config)
        val_dataset = AudioDataset(val_df, config)
        collate_fn = collate_audio
    else:
        raise ValueError(f"Modalità '{config.modality}' non supportata.")

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
    
    if test_df is not None:
        test_dataset = TextDataset(test_df, config, tokenizer) if config.modality == 'text' else AudioDataset(test_df, config)
        test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader, test_loader
        
    return train_loader, val_loader
===== ./src/config.py =====
import yaml
from pathlib import Path
from typing import Any

class Config:
    """
    Classe per caricare la configurazione da un file YAML e permettere
    l'accesso agli attributi tramite dot notation (es. config.data.audio_root).
    """
    def __init__(self, data: dict):
        for key, value in data.items():
            if isinstance(value, dict):
                setattr(self, key, Config(value))
            else:
                setattr(self, key, value)

    def __repr__(self):
        return str(self.__dict__)

    def to_dict(self) -> dict:
        result = {}
        for key, value in self.__dict__.items():
            if isinstance(value, Config):
                result[key] = value.to_dict()
            else:
                result[key] = value
        return result

def load_config(config_path: str | Path) -> Config:
    """Carica un file di configurazione YAML."""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"File di configurazione non trovato in: {path.resolve()}")
        
    with open(path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if not isinstance(config_data, dict):
        raise TypeError("La radice del file YAML deve essere un dizionario.")
        
    return Config(config_data)
===== ./src/utils.py =====
import torch
import numpy as np
import random
import gc

def set_seed(seed: int):
    """Imposta il seed per la riproducibilità."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def clear_memory():
    """Libera la memoria della GPU."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
===== ./src/engine.py =====
import torch
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, classification_report, confusion_matrix

from src.models import build_model
from src.utils import clear_memory

def train_epoch(model, loader, optimizer, scheduler, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in tqdm(loader, desc="Training", leave=False):
        optimizer.zero_grad()
        
        # Sposta il batch sul device corretto
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        
        labels = batch.pop('labels')
        outputs = model(batch)
        
        loss = loss_fn(outputs, labels)
        loss.backward()
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def evaluate_epoch(model, loader, loss_fn, device, task, metric_name):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            labels = batch.pop('labels')
            outputs = model(batch)
            
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            
            if task == 'classification':
                preds = torch.argmax(outputs, dim=1)
            else: # regression
                preds = outputs
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    
    if task == 'classification':
        if metric_name == 'accuracy':
            metric = accuracy_score(all_labels, all_preds)
        elif metric_name == 'f1':
            metric = f1_score(all_labels, all_preds, average='weighted')
        else:
            raise ValueError(f"Metrica '{metric_name}' non supportata per la classificazione.")
    else: # regression
        metric = root_mean_squared_error(all_labels, all_preds) # RMSE
        
    return avg_loss, metric

class Trainer:
    def __init__(self, config, train_loader, val_loader, fold):
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.fold = fold
        self.device = torch.device(config.device)
        self.output_path = Path(config.output_dir) / f"fold_{fold}"
        self.output_path.mkdir(parents=True, exist_ok=True)

    def train(self):
        clear_memory()
        model = build_model(self.config).to(self.device)
        
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        
        num_training_steps = len(self.train_loader) * self.config.training.epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=400, num_training_steps=num_training_steps)
        
        loss_fn = nn.CrossEntropyLoss() if self.config.task == 'classification' else nn.MSELoss()

        best_metric = -np.inf if self.config.training.eval_metric in ['accuracy', 'f1'] else np.inf
        patience_counter = 0

        print(f"--- Inizio Training Fold {self.fold} ---")
        for epoch in range(self.config.training.epochs):
            train_loss = train_epoch(model, self.train_loader, optimizer, scheduler, loss_fn, self.device)
            val_loss, val_metric = evaluate_epoch(model, self.val_loader, loss_fn, self.device, self.config.task, self.config.training.eval_metric)
            
            print(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val {self.config.training.eval_metric}: {val_metric:.4f}")

            is_better = (val_metric > best_metric) if self.config.training.eval_metric in ['accuracy', 'f1'] else (val_metric < best_metric)
            
            if is_better:
                best_metric = val_metric
                torch.save(model.state_dict(), self.output_path / "best_model.pt")
                print(f"-> Modello salvato. Nuova metrica migliore: {best_metric:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config.training.early_stopping_patience:
                print("-> Early stopping attivato.")
                break
        print(f"--- Fine Training Fold {self.fold} ---")

class Predictor:
    def __init__(self, config, test_loader):
        self.config = config
        self.test_loader = test_loader
        self.device = torch.device(config.device)
        self.output_dir = Path(config.output_dir)
    
    def predict(self):
        all_fold_scores = []
        
        for fold in range(self.config.k_folds):
            print(f"--- Inferenza con Fold {fold} ---")
            model_path = self.output_dir / f"fold_{fold}" / "best_model.pt"
            if not model_path.exists():
                print(f"ATTENZIONE: Modello per il fold {fold} non trovato in {model_path}. Salto.")
                continue

            model = build_model(self.config)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.to(self.device)
            model.eval()
            
            fold_scores = []
            test_ids = []
            
            with torch.no_grad():
                for batch in tqdm(self.test_loader, desc=f"Predicting Fold {fold}", leave=False):
                    ids = batch.pop("id")
                    test_ids.extend(ids)
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    outputs = model(batch)
                    
                    if self.config.task == 'classification':
                        scores = torch.softmax(outputs, dim=1)[:, 1] # Probabilità della classe positiva
                    else: # regression
                        scores = outputs
                    
                    fold_scores.extend(scores.cpu().numpy())
            
            all_fold_scores.append(fold_scores)
            
        # Ensembling: media degli score/probabilità
        mean_scores = np.mean(all_fold_scores, axis=0)
        
        # Crea DataFrame finale
        results_df = pd.DataFrame({'ID': test_ids[:len(mean_scores)], 'score': mean_scores})
        
        if self.config.task == 'classification':
            results_df['prediction'] = (results_df['score'] >= 0.5).astype(int)
        else:
            results_df['prediction'] = results_df['score']
            
        output_file = self.output_dir / "predictions.csv"
        results_df[['ID', 'prediction']].to_csv(output_file, index=False)
        print(f"\nPredizioni salvate in {output_file}")
        
class Evaluator:
    def __init__(self, config):
        self.config = config
        self.predictions_path = Path(config.output_dir) / "predictions.csv"
        if config.task == 'classification':
            self.labels_path = Path(config.data.test_task1_labels)
        else:
            self.labels_path = Path(config.data.test_task2_labels)
    
    def evaluate(self):
        if not self.predictions_path.exists():
            raise FileNotFoundError(f"File di predizioni non trovato: {self.predictions_path}")
        
        preds_df = pd.read_csv(self.predictions_path)
        labels_df = pd.read_csv(self.labels_path)
        
        # Assicuriamoci che la colonna ID si chiami allo stesso modo per il merge
        # A volte i CSV hanno nomi leggermente diversi (es. 'id', 'ID', 'adressfname')
        if 'ID' not in labels_df.columns:
             # Cerca una colonna che potrebbe essere l'ID
             possible_id_cols = [col for col in labels_df.columns if 'id' in col.lower() or 'name' in col.lower()]
             if possible_id_cols:
                 labels_df = labels_df.rename(columns={possible_id_cols[0]: 'ID'})
             else:
                 raise ValueError("Non riesco a trovare la colonna ID nel file delle etichette.")

        merged_df = pd.merge(preds_df, labels_df, on="ID")
        
        y_pred = merged_df['prediction']
        
        if self.config.task == 'classification':
            # --- FIX CRUCIALE: MAPPING DELLE ETICHETTE ---
            # Convertiamo le stringhe 'Control'/'ProbableAD' in 0/1
            # Adatta questo dizionario se le tue etichette sono diverse (es. 'CN'/'AD')
            label_mapping = {'Control': 0, 'ProbableAD': 1, 'CN': 0, 'AD': 1}
            
            # Usa la colonna corretta per la diagnosi. Potrebbe chiamarsi 'Dx', 'diagnosis', ecc.
            # Cerchiamo di essere flessibili.
            dx_col = None
            for col in ['Dx', 'diagnosis', 'label']:
                if col in merged_df.columns:
                    dx_col = col
                    break
            
            if dx_col is None:
                 raise ValueError(f"Colonna diagnosi non trovata. Colonne disponibili: {merged_df.columns}")

            # Applica il mapping. Se un valore non è nel dizionario, potrebbe dare errore o NaN.
            try:
                y_true = merged_df[dx_col].map(label_mapping).astype(int)
            except ValueError as e:
                 print(f"Errore durante il mapping delle etichette. Valori unici trovati in {dx_col}: {merged_df[dx_col].unique()}")
                 raise e
            # ---------------------------------------------

            print("----- Valutazione Classificazione -----")
            print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
            print(f"F1-Score (Weighted): {f1_score(y_true, y_pred, average='weighted'):.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true, y_pred, target_names=['CN', 'AD']))
            print("\nConfusion Matrix:")
            print(confusion_matrix(y_true, y_pred))
            print("------------------------------------")
        else: # regression
            y_true = merged_df['MMSE']
            print("----- Valutazione Regressione -----")
            rmse = root_mean_squared_error(y_true, y_pred)
            print(f"RMSE: {rmse:.4f}")
            print("------------------------------------")
===== ./src/__init__.py =====

===== ./scripts/transcribe.py =====
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import torch
import gc

# Aggiunge la root del progetto al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config

# ===================================================================
#      IMPLEMENTAZIONE CORRETTA PER NYRAHEALTH/CRISPERWHISPER
# ===================================================================
def get_crisperwhisper_transcriber(config):
    """
    Prepara la pipeline per CrisperWhisper.
    Carica il token dal file .env solo per questo engine.
    """
    # --- MODIFICA CHIAVE: IMPORT E CARICAMENTO LOCALE ---
    # Questo codice viene eseguito SOLO se l'engine è "crisperwhisper".
    # In questo modo, gli altri ambienti non hanno bisogno di 'python-dotenv'.
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("Info: File .env trovato e caricato per l'autenticazione a Hugging Face.")
    except ImportError:
        print("Info: Libreria 'python-dotenv' non trovata. Si procederà usando il token salvato da 'huggingface-cli login' se disponibile.")
    # --- FINE MODIFICA ---

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    except ImportError:
        print("\nERRORE: La libreria 'transformers' (versione custom) non è installata.")
        print("Assicurati di aver attivato 'crisper_env' e di aver eseguito 'pip install git+...'")
        sys.exit(1)

    cfg = config.transcription.crisperwhisper
    device = config.device
    torch_dtype = torch.float16 if "cuda" in device else torch.float32
    
    print(f"Caricamento backend CrisperWhisper (HF Pipeline) con modello '{cfg.model_id}'...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        cfg.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(cfg.model_id)
    transcription_pipeline = pipeline(
        "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor, chunk_length_s=30,
        batch_size=cfg.batch_size, torch_dtype=torch_dtype, device=device,
    )
    print("Backend CrisperWhisper inizializzato.")

    def transcribe_files(audio_dir, output_dir):
        # ... (Questa logica interna rimane invariata)
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                result = transcription_pipeline(str(audio_path))
                full_text = result["text"].strip()
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files

# ===================================================================
#             LOGICA PER NEMO E WHISPERX (INVARIATE)
# ===================================================================

def get_nemo_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import nemo.collections.asr as nemo_asr
    except ImportError:
        print("\nERRORE: NVIDIA NeMo Toolkit non è installato. Attiva l'ambiente 'nemo_env'.")
        sys.exit(1)
    cfg = config.transcription.nemo
    print(f"Caricamento backend NeMo con modello '{cfg.model_name}'...")
    backend = nemo_asr.models.ASRModel.from_pretrained(model_name=cfg.model_name)
    backend.to(torch.device(config.device))
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o sono già tutti presenti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        str_paths = [str(p) for p in audio_files]
        result = backend.transcribe(paths_to_audio_files=str_paths, batch_size=cfg.batch_size, channel_selector='average', verbose=False)
        transcriptions = result[0] if isinstance(result, tuple) else result
        for audio_path, text_obj in zip(audio_files, transcriptions):
            final_text = text_obj.text if hasattr(text_obj, 'text') else text_obj
            out_path = output_dir / f"{audio_path.stem}.txt"
            out_path.write_text(final_text.strip() if final_text else "", encoding='utf-8')
    return transcribe_files


def get_whisperx_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import whisperx
    except ImportError:
        print("\nERRORE: La libreria 'whisperx' non è installata. Attiva l'ambiente 'whisperx_env'.")
        sys.exit(1)
    cfg = config.transcription.whisperx
    print(f"Caricamento backend WhisperX con modello '{cfg.model_name}'...")
    model = whisperx.load_model(cfg.model_name, config.device, compute_type=cfg.compute_type, language=cfg.language)
    print("Backend WhisperX inizializzato.")
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                audio = whisperx.load_audio(str(audio_path))
                result = model.transcribe(audio, batch_size=cfg.batch_size)
                full_text = " ".join([segment['text'].strip() for segment in result["segments"]])
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files


def main():
    # L'import di dotenv è stato rimosso da qui.
    parser = argparse.ArgumentParser(description="Genera trascrizioni per i dati.")
    # ... (il resto della funzione main rimane identico)
    parser.add_argument("--config", type=str, required=True, help="Percorso al file config.yaml")
    args = parser.parse_args()
    config = load_config(args.config)
    engine = config.transcription.engine.lower()
    transcribe_function = None
    if engine == "crisperwhisper":
        transcribe_function = get_crisperwhisper_transcriber(config)
    elif engine == "nemo":
        transcribe_function = get_nemo_transcriber(config)
    elif engine == "whisperx":
        transcribe_function = get_whisperx_transcriber(config)
    else:
        print(f"ERRORE: Engine '{engine}' non supportato. Scegli tra 'crisperwhisper', 'nemo' o 'whisperx'.")
        sys.exit(1)
    model_folder_name = ""
    if engine == "crisperwhisper":
        model_folder_name = config.transcription.crisperwhisper.model_id.split('/')[-1]
    elif engine == "nemo":
        model_folder_name = config.transcription.nemo.model_name.split('/')[-1]
    elif engine == "whisperx":
        model_folder_name = f"WhisperX_{config.transcription.whisperx.model_name}"
    train_output_dir = Path(config.data.transcripts_root) / model_folder_name
    test_output_dir = Path(config.data.test_transcripts_root) / model_folder_name
    train_output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Gli output verranno salvati in: {train_output_dir} e {test_output_dir}")
    print(f"\n--- Inizio Trascrizione Training Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.audio_root), train_output_dir)
    print(f"\n--- Inizio Trascrizione Test Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.test_audio_root), test_output_dir)
    print("\nTrascrizione completata.")

if __name__ == "__main__":
    main()
===== ./scripts/run.py =====
import argparse
import sys
from pathlib import Path

# Aggiunge la root del progetto al path per permettere 'from src import ...'
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.utils import set_seed
from src.data import get_data_splits, get_dataloaders
from src.engine import Trainer, Predictor, Evaluator

def main():
    parser = argparse.ArgumentParser(description="Script principale per training, predizione e valutazione.")
    parser.add_argument("--config", type=str, required=True, help="Percorso al file di configurazione (es. config.yaml)")
    parser.add_argument("--mode", type=str, required=True, choices=["train", "predict", "evaluate"], help="Modalità di esecuzione.")
    args = parser.parse_args()

    config = load_config(args.config)
    set_seed(config.seed)

    if args.mode == 'train':
        print(f"Inizio training per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Gli output verranno salvati in: {config.output_dir}")
        
        # Il K-Fold è gestito qui
        for fold, train_df, val_df, _ in get_data_splits(config):
            train_loader, val_loader = get_dataloaders(config, train_df, val_df)
            trainer = Trainer(config, train_loader, val_loader, fold)
            trainer.train()
            
    elif args.mode == 'predict':
        print(f"Inizio predizione per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Uso i modelli da: {config.output_dir}")
        
        # Ottieni il test loader (usiamo un ciclo fittizio per prendere il primo test_df)
        for _, _, _, test_df in get_data_splits(config):
            _, _, test_loader = get_dataloaders(config, train_df=test_df, val_df=test_df, test_df=test_df)
            break # Usciamo dopo il primo, dato che test_df è sempre lo stesso
            
        predictor = Predictor(config, test_loader)
        predictor.predict()

    elif args.mode == 'evaluate':
        print(f"Inizio valutazione per il task '{config.task}'.")
        print(f"Valuto le predizioni in: {Path(config.output_dir) / 'predictions.csv'}")
        
        evaluator = Evaluator(config)
        evaluator.evaluate()

if __name__ == "__main__":
    main()
===== ./config.yaml =====
# ===================================================================
#                    CONFIGURAZIONE PRINCIPALE
# ===================================================================

# --- Impostazioni Generali ---
seed: 42
device: "cuda"  # "cuda" o "cpu"

# --- Percorsi dei Dati ---
data:
  audio_root: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio"
  transcripts_root: "transcripts"
  train_labels: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv"
  
  test_audio_root: "Adresso21/ADReSSo21-diagnosis-test/ADReSSo21/diagnosis/test-dist/audio"
  test_transcripts_root: "transcripts_test"
  test_task1_labels: "Adresso21/label_test_task1.csv"
  test_task2_labels: "Adresso21/label_test_task2.csv"

# ===================================================================
#                   CONFIGURAZIONE TRASCRIZIONE (ASR)
# ===================================================================
# Modifica 'engine' per scegliere quale motore ASR usare.
# Lo script leggerà solo la sezione di configurazione corrispondente.
# ===================================================================

transcription:
  # CAMBIA QUI: "crisperwhisper", "nemo", o "whisperx"
  engine: "whisperx"
  overwrite: false

  # --- SEZIONE CRISPERWHISPER RIVISTA E CORRETTA ---
  crisperwhisper:
    # L'unico parametro che ci serve è l'ID del modello su Hugging Face
    model_id: "nyrahealth/CrisperWhisper" 
    batch_size: 16
    compute_type: "float16" # o "int8"
    
  # --- Impostazioni per NeMo ---
  nemo:
    model_name: "nvidia/parakeet-tdt-0.6b-v2"
    batch_size: 16

  # --- Impostazioni per WhisperX ---
  whisperx:
    model_name: "nyrahealth/faster_CrisperWhisper"
    batch_size: 16
    compute_type: "float16"
    language: "en"
# ===================================================================
#                     CONFIGURAZIONE ESPERIMENTO
# ===================================================================
# Questa sezione controlla le operazioni di training, predizione e valutazione
# lanciate con 'python scripts/run.py'.
# ===================================================================

# --- Task e Modello ---
task: "classification"    # "classification" o "regression"
modality: "audio"          # "text" o "audio"

# Modello di trascrizione da usare per l'addestramento del modello testuale.
# Deve corrispondere al nome della cartella creata da 'transcribe.py'.
# Esempio per Crisper: "Crisper_whisper-large-v3"
# Esempio per NeMo: "parakeet-tdt-0.6b-v2"
transcription_model_for_training: "large-v3"

# --- Cross-Validation ---
k_folds: 5
output_dir: "outputs/bert_classification_parakeet" # Cartella dove salvare modelli e predizioni

# --- Parametri del Modello ---
model:
  text:
    name: "bert-large-uncased"
    max_length: 275
    dropout: 0.1

  audio:
    name: "ecapa-tdnn"
    pretrained: "speechbrain/spkrec-ecapa-voxceleb"
    sample_rate: 16000
    trainable_encoder: true 
    dropout: 0.1

# --- Parametri di Addestramento ---
training:
  epochs: 15
  batch_size: 1
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 400
  early_stopping_patience: 3
  eval_metric: "accuracy" # Per classificazione: "accuracy" o "f1". Per regressione: "rmse".
===== ./src/models.py =====
import torch
import torch.nn as nn
from transformers import AutoModel
from speechbrain.pretrained import EncoderClassifier
import sys

class BertClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = AutoModel.from_pretrained(config.model.text.name)
        self.dropout = nn.Dropout(config.model.text.dropout)
        num_classes = 2 if config.task == 'classification' else 1
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
        nn.init.xavier_uniform_(self.classifier.weight)
        if self.classifier.bias is not None:
            nn.init.zeros_(self.classifier.bias)
            
    def forward(self, batch):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        if self.classifier.out_features > 1:
            return logits
        else:
            return logits.squeeze(-1)

class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.device = torch.device(config.device)

        # Carica l'encoder direttamente sul device corretto (CRUCIALE)
        self.encoder = EncoderClassifier.from_hparams(
            source=config.model.audio.pretrained,
            run_opts={"device": config.device}
        )

        # Congelamento opzionale
        if not config.model.audio.trainable_encoder:
            for p in self.encoder.parameters():
                p.requires_grad = False

        # Ricava la dim. embedding in modo robusto usando un dummy SULLO STESSO DEVICE
        with torch.no_grad():
            dummy = torch.zeros(1, config.model.audio.sample_rate, device=self.device)
            emb = self.encoder.encode_batch(dummy)     # [1,1,D] o [1,D]
            if emb.dim() == 3 and emb.size(1) == 1:
                emb = emb.squeeze(1)
            num_features = emb.shape[-1]

        num_classes = 2 if config.task == 'classification' else 1
        self.classifier_head = nn.Sequential(
            nn.Linear(num_features, num_features // 2),
            nn.ReLU(),
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features // 2, num_classes),
        )
        for m in self.classifier_head.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, batch):
        waveforms = batch['waveform']
        
        # --- DEBUG START ---
        device = next(self.parameters()).device
        print(f"\n--- INIZIO FORWARD PASS ---")
        print(f"1. Dispositivo del modello (atteso: cuda): {device}")
        print(f"2. Dispositivo waveforms in input (atteso: cuda): {waveforms.device}")

        # La funzione encode_batch di Speechbrain è una scatola nera.
        # Apriamola per vedere cosa succede dentro.
        try:
            # Step 1: Calcolo delle features (es. MFCC)
            feats = self.encoder.mods.compute_features(waveforms)
            print(f"3. Dispositivo delle features calcolate (ATTESO: cuda): {feats.device} <--- QUESTO È IL PUNTO CRITICO")

            # Step 2: Passaggio nel modello di embedding
            embedding_model_device = next(self.encoder.mods.embedding_model.parameters()).device
            print(f"4. Dispositivo del modello di embedding (atteso: cuda): {embedding_model_device}")
            embeddings = self.encoder.mods.embedding_model(feats)
            print(f"5. Dispositivo degli embedding finali (atteso: cuda): {embeddings.device}")

        except Exception as e:
            print("\nERRORE DURANTE IL DEBUG:")
            print(f"L'errore è avvenuto qui: {e}")
            # Forziamo un controllo dei dispositivi per capire lo stato al momento del crash
            print(f"Stato al crash - Input: {waveforms.device}, Modello Embedding: {next(self.encoder.mods.embedding_model.parameters()).device}")
            raise e
        # --- DEBUG END ---

        embeddings = embeddings.squeeze(1) 
        outputs = self.classifier_head(embeddings)
        
        if self.classifier_head[-1].out_features > 1:
            return outputs
        else:
            return outputs.squeeze(-1)
        
def build_model(config):
    if config.modality == 'text':
        return BertClassifier(config)
    elif config.modality == 'audio':
        return EcapaTdnnClassifier(config)
    else:
        raise ValueError(f"Modello per la modalità '{config.modality}' non supportato.")
===== ./src/data.py =====
from pathlib import Path
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from sklearn.model_selection import StratifiedKFold
import torchaudio
from torchaudio.functional import resample

class TextDataset(Dataset):
    # --- QUESTA CLASSE È CORRETTA E RIMANE INVARIATA ---
    def __init__(self, df: pd.DataFrame, config, tokenizer):
        self.df = df; self.config = config; self.tokenizer = tokenizer; self.is_test = 'label' not in df.columns
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        transcript_dir = Path(self.config.data.transcripts_root if not self.is_test else self.config.data.test_transcripts_root)
        transcript_path = transcript_dir / self.config.transcription_model_for_training / f"{file_id}.txt"
        try: text = transcript_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError: text = ""
        inputs = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.config.model.text.max_length,
            padding="max_length", truncation=True, return_attention_mask=True, return_tensors="pt",
        )
        item = {"input_ids": inputs["input_ids"].flatten(), "attention_mask": inputs["attention_mask"].flatten(), "id": file_id}
        if not self.is_test:
            item['labels'] = torch.tensor(row['label'], dtype=torch.long if self.config.task == 'classification' else torch.float32)
        return item

class AudioDataset(Dataset):
    def __init__(self, df: pd.DataFrame, config):
        self.df = df
        self.config = config
        self.is_test = 'label' not in df.columns
        self.audio_root = Path(self.config.data.audio_root if not self.is_test else self.config.data.test_audio_root)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        audio_path = self.audio_root / f"{file_id}.wav"
        if not audio_path.exists() and not self.is_test:
            group = 'ad' if row['label'] == 1 else 'cn'
            audio_path = self.audio_root / group / f"{file_id}.wav"

        waveform, sr = torchaudio.load(audio_path)
        
        # --- FIX #1: FORZARE L'AUDIO MONO ---
        # Se la waveform ha più di un canale (es. è stereo), ne facciamo la media
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        # ------------------------------------

        if sr != self.config.model.audio.sample_rate:
            waveform = resample(waveform, sr, self.config.model.audio.sample_rate)
        
        # Ora .squeeze() funzionerà sempre, perché l'input è sempre [1, n_samples]
        item = {"waveform": waveform.squeeze(0), "id": file_id}

        if not self.is_test:
            target = row['label']
            dtype = torch.long if self.config.task == 'classification' else torch.float32
            item['labels'] = torch.tensor(target, dtype=dtype)
            
        return item

def collate_audio(batch):
    waveforms = [item['waveform'] for item in batch]   # ciascuno [T]
    ids = [item['id'] for item in batch]
    lengths = [w.shape[-1] for w in waveforms]

    padded = pad_sequence(waveforms, batch_first=True, padding_value=0.0)  # [B, T]
    max_len = padded.shape[1]
    wav_lens = torch.tensor([L / max_len for L in lengths], dtype=torch.float32)

    collated = {"waveform": padded, "lengths": wav_lens, "id": ids}
    if 'labels' in batch[0]:
        collated['labels'] = torch.stack([it['labels'] for it in batch])
    return collated



def get_data_splits(config):
    """Prepara i DataFrame per training, validazione e test."""
    train_df = pd.read_csv(config.data.train_labels)
    train_df = train_df.rename(columns={'adressfname': 'ID', 'dx': 'diagnosis'})
    train_df = train_df.set_index('ID')
    
    if config.task == 'classification':
        train_df['label'] = train_df['diagnosis'].apply(lambda x: 1 if x == 'ad' else 0)
    else:
        train_df['label'] = train_df['mmse']
        
    # --- FIX #2: RIMOSSO .sort_index() ERRATO ---
    # Mescoliamo il dataframe una volta per sicurezza, senza riordinarlo.
    train_df = train_df.sample(frac=1, random_state=config.seed)
    # ---------------------------------------------
        
    test_df = pd.read_csv(config.data.test_task1_labels).rename(columns={'ID': 'id'}).set_index('id')

    skf = StratifiedKFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):
        train_split = train_df.iloc[train_idx]
        val_split = train_df.iloc[val_idx]
        yield fold, train_split, val_split, test_df
        
def get_dataloaders(config, train_df, val_df, test_df=None):
    if config.modality == 'text':
        tokenizer = AutoTokenizer.from_pretrained(config.model.text.name)
        train_dataset = TextDataset(train_df, config, tokenizer)
        val_dataset = TextDataset(val_df, config, tokenizer)
        collate_fn = None
    elif config.modality == 'audio':
        train_dataset = AudioDataset(train_df, config)
        val_dataset = AudioDataset(val_df, config)
        collate_fn = collate_audio
    else:
        raise ValueError(f"Modalità '{config.modality}' non supportata.")

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
    
    if test_df is not None:
        test_dataset = TextDataset(test_df, config, tokenizer) if config.modality == 'text' else AudioDataset(test_df, config)
        test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader, test_loader
        
    return train_loader, val_loader
===== ./src/config.py =====
import yaml
from pathlib import Path
from typing import Any

class Config:
    """
    Classe per caricare la configurazione da un file YAML e permettere
    l'accesso agli attributi tramite dot notation (es. config.data.audio_root).
    """
    def __init__(self, data: dict):
        for key, value in data.items():
            if isinstance(value, dict):
                setattr(self, key, Config(value))
            else:
                setattr(self, key, value)

    def __repr__(self):
        return str(self.__dict__)

    def to_dict(self) -> dict:
        result = {}
        for key, value in self.__dict__.items():
            if isinstance(value, Config):
                result[key] = value.to_dict()
            else:
                result[key] = value
        return result

def load_config(config_path: str | Path) -> Config:
    """Carica un file di configurazione YAML."""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"File di configurazione non trovato in: {path.resolve()}")
        
    with open(path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if not isinstance(config_data, dict):
        raise TypeError("La radice del file YAML deve essere un dizionario.")
        
    return Config(config_data)
===== ./src/utils.py =====
import torch
import numpy as np
import random
import gc

def set_seed(seed: int):
    """Imposta il seed per la riproducibilità."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def clear_memory():
    """Libera la memoria della GPU."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
===== ./src/engine.py =====
import torch
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, classification_report, confusion_matrix

from src.models import build_model
from src.utils import clear_memory

def train_epoch(model, loader, optimizer, scheduler, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in tqdm(loader, desc="Training", leave=False):
        optimizer.zero_grad()
        
        # Sposta il batch sul device corretto
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        
        labels = batch.pop('labels')
        outputs = model(batch)
        
        loss = loss_fn(outputs, labels)
        loss.backward()
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def evaluate_epoch(model, loader, loss_fn, device, task, metric_name):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            labels = batch.pop('labels')
            outputs = model(batch)
            
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            
            if task == 'classification':
                preds = torch.argmax(outputs, dim=1)
            else: # regression
                preds = outputs
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    
    if task == 'classification':
        if metric_name == 'accuracy':
            metric = accuracy_score(all_labels, all_preds)
        elif metric_name == 'f1':
            metric = f1_score(all_labels, all_preds, average='weighted')
        else:
            raise ValueError(f"Metrica '{metric_name}' non supportata per la classificazione.")
    else: # regression
        metric = root_mean_squared_error(all_labels, all_preds) # RMSE
        
    return avg_loss, metric

class Trainer:
    def __init__(self, config, train_loader, val_loader, fold):
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.fold = fold
        self.device = torch.device(config.device)
        self.output_path = Path(config.output_dir) / f"fold_{fold}"
        self.output_path.mkdir(parents=True, exist_ok=True)

    def train(self):
        clear_memory()
        model = build_model(self.config).to(self.device)
        
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        
        num_training_steps = len(self.train_loader) * self.config.training.epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=400, num_training_steps=num_training_steps)
        
        loss_fn = nn.CrossEntropyLoss() if self.config.task == 'classification' else nn.MSELoss()

        best_metric = -np.inf if self.config.training.eval_metric in ['accuracy', 'f1'] else np.inf
        patience_counter = 0

        print(f"--- Inizio Training Fold {self.fold} ---")
        for epoch in range(self.config.training.epochs):
            train_loss = train_epoch(model, self.train_loader, optimizer, scheduler, loss_fn, self.device)
            val_loss, val_metric = evaluate_epoch(model, self.val_loader, loss_fn, self.device, self.config.task, self.config.training.eval_metric)
            
            print(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val {self.config.training.eval_metric}: {val_metric:.4f}")

            is_better = (val_metric > best_metric) if self.config.training.eval_metric in ['accuracy', 'f1'] else (val_metric < best_metric)
            
            if is_better:
                best_metric = val_metric
                torch.save(model.state_dict(), self.output_path / "best_model.pt")
                print(f"-> Modello salvato. Nuova metrica migliore: {best_metric:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config.training.early_stopping_patience:
                print("-> Early stopping attivato.")
                break
        print(f"--- Fine Training Fold {self.fold} ---")

class Predictor:
    def __init__(self, config, test_loader):
        self.config = config
        self.test_loader = test_loader
        self.device = torch.device(config.device)
        self.output_dir = Path(config.output_dir)
    
    def predict(self):
        all_fold_scores = []
        
        for fold in range(self.config.k_folds):
            print(f"--- Inferenza con Fold {fold} ---")
            model_path = self.output_dir / f"fold_{fold}" / "best_model.pt"
            if not model_path.exists():
                print(f"ATTENZIONE: Modello per il fold {fold} non trovato in {model_path}. Salto.")
                continue

            model = build_model(self.config)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.to(self.device)
            model.eval()
            
            fold_scores = []
            test_ids = []
            
            with torch.no_grad():
                for batch in tqdm(self.test_loader, desc=f"Predicting Fold {fold}", leave=False):
                    ids = batch.pop("id")
                    test_ids.extend(ids)
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    outputs = model(batch)
                    
                    if self.config.task == 'classification':
                        scores = torch.softmax(outputs, dim=1)[:, 1] # Probabilità della classe positiva
                    else: # regression
                        scores = outputs
                    
                    fold_scores.extend(scores.cpu().numpy())
            
            all_fold_scores.append(fold_scores)
            
        # Ensembling: media degli score/probabilità
        mean_scores = np.mean(all_fold_scores, axis=0)
        
        # Crea DataFrame finale
        results_df = pd.DataFrame({'ID': test_ids[:len(mean_scores)], 'score': mean_scores})
        
        if self.config.task == 'classification':
            results_df['prediction'] = (results_df['score'] >= 0.5).astype(int)
        else:
            results_df['prediction'] = results_df['score']
            
        output_file = self.output_dir / "predictions.csv"
        results_df[['ID', 'prediction']].to_csv(output_file, index=False)
        print(f"\nPredizioni salvate in {output_file}")
        
class Evaluator:
    def __init__(self, config):
        self.config = config
        self.predictions_path = Path(config.output_dir) / "predictions.csv"
        if config.task == 'classification':
            self.labels_path = Path(config.data.test_task1_labels)
        else:
            self.labels_path = Path(config.data.test_task2_labels)
    
    def evaluate(self):
        if not self.predictions_path.exists():
            raise FileNotFoundError(f"File di predizioni non trovato: {self.predictions_path}")
        
        preds_df = pd.read_csv(self.predictions_path)
        labels_df = pd.read_csv(self.labels_path)
        
        # Assicuriamoci che la colonna ID si chiami allo stesso modo per il merge
        # A volte i CSV hanno nomi leggermente diversi (es. 'id', 'ID', 'adressfname')
        if 'ID' not in labels_df.columns:
             # Cerca una colonna che potrebbe essere l'ID
             possible_id_cols = [col for col in labels_df.columns if 'id' in col.lower() or 'name' in col.lower()]
             if possible_id_cols:
                 labels_df = labels_df.rename(columns={possible_id_cols[0]: 'ID'})
             else:
                 raise ValueError("Non riesco a trovare la colonna ID nel file delle etichette.")

        merged_df = pd.merge(preds_df, labels_df, on="ID")
        
        y_pred = merged_df['prediction']
        
        if self.config.task == 'classification':
            # --- FIX CRUCIALE: MAPPING DELLE ETICHETTE ---
            # Convertiamo le stringhe 'Control'/'ProbableAD' in 0/1
            # Adatta questo dizionario se le tue etichette sono diverse (es. 'CN'/'AD')
            label_mapping = {'Control': 0, 'ProbableAD': 1, 'CN': 0, 'AD': 1}
            
            # Usa la colonna corretta per la diagnosi. Potrebbe chiamarsi 'Dx', 'diagnosis', ecc.
            # Cerchiamo di essere flessibili.
            dx_col = None
            for col in ['Dx', 'diagnosis', 'label']:
                if col in merged_df.columns:
                    dx_col = col
                    break
            
            if dx_col is None:
                 raise ValueError(f"Colonna diagnosi non trovata. Colonne disponibili: {merged_df.columns}")

            # Applica il mapping. Se un valore non è nel dizionario, potrebbe dare errore o NaN.
            try:
                y_true = merged_df[dx_col].map(label_mapping).astype(int)
            except ValueError as e:
                 print(f"Errore durante il mapping delle etichette. Valori unici trovati in {dx_col}: {merged_df[dx_col].unique()}")
                 raise e
            # ---------------------------------------------

            print("----- Valutazione Classificazione -----")
            print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
            print(f"F1-Score (Weighted): {f1_score(y_true, y_pred, average='weighted'):.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true, y_pred, target_names=['CN', 'AD']))
            print("\nConfusion Matrix:")
            print(confusion_matrix(y_true, y_pred))
            print("------------------------------------")
        else: # regression
            y_true = merged_df['MMSE']
            print("----- Valutazione Regressione -----")
            rmse = root_mean_squared_error(y_true, y_pred)
            print(f"RMSE: {rmse:.4f}")
            print("------------------------------------")
===== ./src/__init__.py =====

===== ./scripts/transcribe.py =====
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import torch
import gc

# Aggiunge la root del progetto al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config

# ===================================================================
#      IMPLEMENTAZIONE CORRETTA PER NYRAHEALTH/CRISPERWHISPER
# ===================================================================
def get_crisperwhisper_transcriber(config):
    """
    Prepara la pipeline per CrisperWhisper.
    Carica il token dal file .env solo per questo engine.
    """
    # --- MODIFICA CHIAVE: IMPORT E CARICAMENTO LOCALE ---
    # Questo codice viene eseguito SOLO se l'engine è "crisperwhisper".
    # In questo modo, gli altri ambienti non hanno bisogno di 'python-dotenv'.
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("Info: File .env trovato e caricato per l'autenticazione a Hugging Face.")
    except ImportError:
        print("Info: Libreria 'python-dotenv' non trovata. Si procederà usando il token salvato da 'huggingface-cli login' se disponibile.")
    # --- FINE MODIFICA ---

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    except ImportError:
        print("\nERRORE: La libreria 'transformers' (versione custom) non è installata.")
        print("Assicurati di aver attivato 'crisper_env' e di aver eseguito 'pip install git+...'")
        sys.exit(1)

    cfg = config.transcription.crisperwhisper
    device = config.device
    torch_dtype = torch.float16 if "cuda" in device else torch.float32
    
    print(f"Caricamento backend CrisperWhisper (HF Pipeline) con modello '{cfg.model_id}'...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        cfg.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(cfg.model_id)
    transcription_pipeline = pipeline(
        "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor, chunk_length_s=30,
        batch_size=cfg.batch_size, torch_dtype=torch_dtype, device=device,
    )
    print("Backend CrisperWhisper inizializzato.")

    def transcribe_files(audio_dir, output_dir):
        # ... (Questa logica interna rimane invariata)
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                result = transcription_pipeline(str(audio_path))
                full_text = result["text"].strip()
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files

# ===================================================================
#             LOGICA PER NEMO E WHISPERX (INVARIATE)
# ===================================================================

def get_nemo_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import nemo.collections.asr as nemo_asr
    except ImportError:
        print("\nERRORE: NVIDIA NeMo Toolkit non è installato. Attiva l'ambiente 'nemo_env'.")
        sys.exit(1)
    cfg = config.transcription.nemo
    print(f"Caricamento backend NeMo con modello '{cfg.model_name}'...")
    backend = nemo_asr.models.ASRModel.from_pretrained(model_name=cfg.model_name)
    backend.to(torch.device(config.device))
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o sono già tutti presenti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        str_paths = [str(p) for p in audio_files]
        result = backend.transcribe(paths_to_audio_files=str_paths, batch_size=cfg.batch_size, channel_selector='average', verbose=False)
        transcriptions = result[0] if isinstance(result, tuple) else result
        for audio_path, text_obj in zip(audio_files, transcriptions):
            final_text = text_obj.text if hasattr(text_obj, 'text') else text_obj
            out_path = output_dir / f"{audio_path.stem}.txt"
            out_path.write_text(final_text.strip() if final_text else "", encoding='utf-8')
    return transcribe_files


def get_whisperx_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import whisperx
    except ImportError:
        print("\nERRORE: La libreria 'whisperx' non è installata. Attiva l'ambiente 'whisperx_env'.")
        sys.exit(1)
    cfg = config.transcription.whisperx
    print(f"Caricamento backend WhisperX con modello '{cfg.model_name}'...")
    model = whisperx.load_model(cfg.model_name, config.device, compute_type=cfg.compute_type, language=cfg.language)
    print("Backend WhisperX inizializzato.")
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                audio = whisperx.load_audio(str(audio_path))
                result = model.transcribe(audio, batch_size=cfg.batch_size)
                full_text = " ".join([segment['text'].strip() for segment in result["segments"]])
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files


def main():
    # L'import di dotenv è stato rimosso da qui.
    parser = argparse.ArgumentParser(description="Genera trascrizioni per i dati.")
    # ... (il resto della funzione main rimane identico)
    parser.add_argument("--config", type=str, required=True, help="Percorso al file config.yaml")
    args = parser.parse_args()
    config = load_config(args.config)
    engine = config.transcription.engine.lower()
    transcribe_function = None
    if engine == "crisperwhisper":
        transcribe_function = get_crisperwhisper_transcriber(config)
    elif engine == "nemo":
        transcribe_function = get_nemo_transcriber(config)
    elif engine == "whisperx":
        transcribe_function = get_whisperx_transcriber(config)
    else:
        print(f"ERRORE: Engine '{engine}' non supportato. Scegli tra 'crisperwhisper', 'nemo' o 'whisperx'.")
        sys.exit(1)
    model_folder_name = ""
    if engine == "crisperwhisper":
        model_folder_name = config.transcription.crisperwhisper.model_id.split('/')[-1]
    elif engine == "nemo":
        model_folder_name = config.transcription.nemo.model_name.split('/')[-1]
    elif engine == "whisperx":
        model_folder_name = f"WhisperX_{config.transcription.whisperx.model_name}"
    train_output_dir = Path(config.data.transcripts_root) / model_folder_name
    test_output_dir = Path(config.data.test_transcripts_root) / model_folder_name
    train_output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Gli output verranno salvati in: {train_output_dir} e {test_output_dir}")
    print(f"\n--- Inizio Trascrizione Training Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.audio_root), train_output_dir)
    print(f"\n--- Inizio Trascrizione Test Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.test_audio_root), test_output_dir)
    print("\nTrascrizione completata.")

if __name__ == "__main__":
    main()
===== ./scripts/run.py =====
import argparse
import sys
from pathlib import Path

# Aggiunge la root del progetto al path per permettere 'from src import ...'
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.utils import set_seed
from src.data import get_data_splits, get_dataloaders
from src.engine import Trainer, Predictor, Evaluator

def main():
    parser = argparse.ArgumentParser(description="Script principale per training, predizione e valutazione.")
    parser.add_argument("--config", type=str, required=True, help="Percorso al file di configurazione (es. config.yaml)")
    parser.add_argument("--mode", type=str, required=True, choices=["train", "predict", "evaluate"], help="Modalità di esecuzione.")
    args = parser.parse_args()

    config = load_config(args.config)
    set_seed(config.seed)

    if args.mode == 'train':
        print(f"Inizio training per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Gli output verranno salvati in: {config.output_dir}")
        
        # Il K-Fold è gestito qui
        for fold, train_df, val_df, _ in get_data_splits(config):
            train_loader, val_loader = get_dataloaders(config, train_df, val_df)
            trainer = Trainer(config, train_loader, val_loader, fold)
            trainer.train()
            
    elif args.mode == 'predict':
        print(f"Inizio predizione per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Uso i modelli da: {config.output_dir}")
        
        # Ottieni il test loader (usiamo un ciclo fittizio per prendere il primo test_df)
        for _, _, _, test_df in get_data_splits(config):
            _, _, test_loader = get_dataloaders(config, train_df=test_df, val_df=test_df, test_df=test_df)
            break # Usciamo dopo il primo, dato che test_df è sempre lo stesso
            
        predictor = Predictor(config, test_loader)
        predictor.predict()

    elif args.mode == 'evaluate':
        print(f"Inizio valutazione per il task '{config.task}'.")
        print(f"Valuto le predizioni in: {Path(config.output_dir) / 'predictions.csv'}")
        
        evaluator = Evaluator(config)
        evaluator.evaluate()

if __name__ == "__main__":
    main()
===== ./config.yaml =====
# ===================================================================
#                    CONFIGURAZIONE PRINCIPALE
# ===================================================================

# --- Impostazioni Generali ---
seed: 42
device: "cuda"  # "cuda" o "cpu"

# --- Percorsi dei Dati ---
data:
  audio_root: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio"
  transcripts_root: "transcripts"
  train_labels: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv"
  
  test_audio_root: "Adresso21/ADReSSo21-diagnosis-test/ADReSSo21/diagnosis/test-dist/audio"
  test_transcripts_root: "transcripts_test"
  test_task1_labels: "Adresso21/label_test_task1.csv"
  test_task2_labels: "Adresso21/label_test_task2.csv"

# ===================================================================
#                   CONFIGURAZIONE TRASCRIZIONE (ASR)
# ===================================================================
# Modifica 'engine' per scegliere quale motore ASR usare.
# Lo script leggerà solo la sezione di configurazione corrispondente.
# ===================================================================

transcription:
  # CAMBIA QUI: "crisperwhisper", "nemo", o "whisperx"
  engine: "whisperx"
  overwrite: false

  # --- SEZIONE CRISPERWHISPER RIVISTA E CORRETTA ---
  crisperwhisper:
    # L'unico parametro che ci serve è l'ID del modello su Hugging Face
    model_id: "nyrahealth/CrisperWhisper" 
    batch_size: 16
    compute_type: "float16" # o "int8"
    
  # --- Impostazioni per NeMo ---
  nemo:
    model_name: "nvidia/parakeet-tdt-0.6b-v2"
    batch_size: 16

  # --- Impostazioni per WhisperX ---
  whisperx:
    model_name: "nyrahealth/faster_CrisperWhisper"
    batch_size: 16
    compute_type: "float16"
    language: "en"
# ===================================================================
#                     CONFIGURAZIONE ESPERIMENTO
# ===================================================================
# Questa sezione controlla le operazioni di training, predizione e valutazione
# lanciate con 'python scripts/run.py'.
# ===================================================================

# --- Task e Modello ---
task: "classification"    # "classification" o "regression"
modality: "audio"          # "text" o "audio"

# Modello di trascrizione da usare per l'addestramento del modello testuale.
# Deve corrispondere al nome della cartella creata da 'transcribe.py'.
# Esempio per Crisper: "Crisper_whisper-large-v3"
# Esempio per NeMo: "parakeet-tdt-0.6b-v2"
transcription_model_for_training: "large-v3"

# --- Cross-Validation ---
k_folds: 5
output_dir: "outputs/bert_classification_parakeet" # Cartella dove salvare modelli e predizioni

# --- Parametri del Modello ---
model:
  text:
    name: "bert-large-uncased"
    max_length: 275
    dropout: 0.1

  audio:
    name: "ecapa-tdnn"
    pretrained: "speechbrain/spkrec-ecapa-voxceleb"
    sample_rate: 16000
    trainable_encoder: true 
    dropout: 0.1

# --- Parametri di Addestramento ---
training:
  epochs: 15
  batch_size: 4 
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 400
  early_stopping_patience: 3
  eval_metric: "accuracy" # Per classificazione: "accuracy" o "f1". Per regressione: "rmse".
===== ./src/models.py =====
import torch
import torch.nn as nn
from transformers import AutoModel
from speechbrain.pretrained import EncoderClassifier
import sys
# ... (la classe BertClassifier rimane invariata) ...
class BertClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = AutoModel.from_pretrained(config.model.text.name)
        self.dropout = nn.Dropout(config.model.text.dropout)
        num_classes = 2 if config.task == 'classification' else 1
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, batch):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        if self.classifier.out_features > 1:
            return logits
        else:
            return logits.squeeze(-1)


class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        # --- FIX: Carica l'encoder direttamente sul device specificato in config.yaml ---
        # Questo è il metodo più sicuro per i modelli pre-addestrati complessi.
        self.encoder = EncoderClassifier.from_hparams(
            source=config.model.audio.pretrained,
            run_opts={"device": config.device}
        )
        
        if not config.model.audio.trainable_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = False

        # Ottiene la dimensione dell'embedding in modo sicuro
        with torch.no_grad():
            # Assicurati che anche il tensore fittizio sia sul device corretto
            dummy_input = torch.zeros(1, config.model.audio.sample_rate, device=config.device)
            dummy_embedding = self.encoder.encode_batch(dummy_input)
            num_features = dummy_embedding.shape[-1]
        
        num_classes = 2 if config.task == 'classification' else 1
        
        self.classifier_head = nn.Sequential(
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features, num_classes)
        )

    def forward(self, batch):
        waveforms = batch['waveform']
        
        is_trainable = any(p.requires_grad for p in self.encoder.parameters())
        with torch.set_grad_enabled(is_trainable):
            # Non è necessario passare 'lengths', encode_batch gestisce il padding
            embeddings = self.encoder.encode_batch(waveforms)

        embeddings = embeddings.squeeze(1) 
        outputs = self.classifier_head(embeddings)
        
        if self.classifier_head[-1].out_features > 1:
            return outputs
        else:
            return outputs.squeeze(-1)

def build_model(config):
    if config.modality == 'text':
        return BertClassifier(config)
    elif config.modality == 'audio':
        return EcapaTdnnClassifier(config)
    else:
        raise ValueError(f"Modello per la modalità '{config.modality}' non supportato.")
def bert_head_init(m: nn.Module, std: float = 0.02):
    if isinstance(m, nn.Linear):
        if hasattr(nn.init, "trunc_normal_"):
            nn.init.trunc_normal_(m.weight, mean=0.0, std=std, a=-2*std, b=2*std)
        else:
            # fallback: normal + clamp (~truncated)
            with torch.no_grad():
                m.weight.normal_(0.0, std).clamp_(-2*std, 2*std)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
===== ./src/data.py =====
from pathlib import Path
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from sklearn.model_selection import StratifiedKFold
import torchaudio
from torchaudio.functional import resample

class TextDataset(Dataset):
    # --- QUESTA CLASSE È CORRETTA E RIMANE INVARIATA ---
    def __init__(self, df: pd.DataFrame, config, tokenizer):
        self.df = df; self.config = config; self.tokenizer = tokenizer; self.is_test = 'label' not in df.columns
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        transcript_dir = Path(self.config.data.transcripts_root if not self.is_test else self.config.data.test_transcripts_root)
        transcript_path = transcript_dir / self.config.transcription_model_for_training / f"{file_id}.txt"
        try: text = transcript_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError: text = ""
        inputs = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.config.model.text.max_length,
            padding="max_length", truncation=True, return_attention_mask=True, return_tensors="pt",
        )
        item = {"input_ids": inputs["input_ids"].flatten(), "attention_mask": inputs["attention_mask"].flatten(), "id": file_id}
        if not self.is_test:
            item['labels'] = torch.tensor(row['label'], dtype=torch.long if self.config.task == 'classification' else torch.float32)
        return item

class AudioDataset(Dataset):
    def __init__(self, df: pd.DataFrame, config):
        self.df = df
        self.config = config
        self.is_test = 'label' not in df.columns
        self.audio_root = Path(self.config.data.audio_root if not self.is_test else self.config.data.test_audio_root)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        audio_path = self.audio_root / f"{file_id}.wav"
        if not audio_path.exists() and not self.is_test:
            group = 'ad' if row['label'] == 1 else 'cn'
            audio_path = self.audio_root / group / f"{file_id}.wav"

        waveform, sr = torchaudio.load(audio_path)
        
        # --- FIX #1: FORZARE L'AUDIO MONO ---
        # Se la waveform ha più di un canale (es. è stereo), ne facciamo la media
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        # ------------------------------------

        if sr != self.config.model.audio.sample_rate:
            waveform = resample(waveform, sr, self.config.model.audio.sample_rate)
        
        # Ora .squeeze() funzionerà sempre, perché l'input è sempre [1, n_samples]
        item = {"waveform": waveform.squeeze(0), "id": file_id}

        if not self.is_test:
            target = row['label']
            dtype = torch.long if self.config.task == 'classification' else torch.float32
            item['labels'] = torch.tensor(target, dtype=dtype)
            
        return item

def collate_audio(batch):
    waveforms = [item['waveform'] for item in batch]
    ids = [item['id'] for item in batch]
    
    # Eseguiamo il padding delle waveform
    padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)
    
    # --- FIX: Rimuoviamo il calcolo delle 'lengths' per semplicità ---
    collated_batch = {"waveform": padded_waveforms, "id": ids}
    
    if 'labels' in batch[0]:
        labels = torch.stack([item['labels'] for item in batch])
        collated_batch['labels'] = labels
        
    return collated_batch



def get_data_splits(config):
    """Prepara i DataFrame per training, validazione e test."""
    train_df = pd.read_csv(config.data.train_labels)
    # Rinominiamo la colonna ID in modo standard
    train_df = train_df.rename(columns={'adressfname': 'ID'})
    train_df = train_df.set_index('ID')
    
    # --- FIX CRUCIALE: LOGICA ROBUSTA PER TROVARE LA COLONNA DELLA DIAGNOSI ---
    diagnosis_col = None
    possible_cols = ['dx', 'diagnosis', 'Dx'] # Lista di nomi di colonna comuni per la diagnosi
    for col in possible_cols:
        if col in train_df.columns:
            diagnosis_col = col
            break
            
    if diagnosis_col is None:
        raise ValueError(f"Impossibile trovare la colonna della diagnosi. Colonne disponibili: {train_df.columns}")
        
    print(f"Info: Trovata colonna diagnosi '{diagnosis_col}'. Verrà usata per creare le etichette.")
    # --- FINE FIX ---

    if config.task == 'classification':
        # Usiamo la colonna trovata per creare le etichette
        train_df['label'] = train_df[diagnosis_col].apply(lambda x: 1 if x == 'ad' else 0)
    else:
        # Per la regressione, assumiamo che la colonna mmse esista
        if 'mmse' not in train_df.columns:
            raise ValueError(f"Task di regressione selezionato ma colonna 'mmse' non trovata. Colonne: {train_df.columns}")
        train_df['label'] = train_df['mmse']
        
    train_df = train_df.sample(frac=1, random_state=config.seed)
    
    test_df = pd.read_csv(config.data.test_task1_labels).rename(columns={'ID': 'id'}).set_index('id')

    skf = StratifiedKFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):
        train_split = train_df.iloc[train_idx]
        val_split = train_df.iloc[val_idx]
        yield fold, train_split, val_split, test_df
        
def get_dataloaders(config, train_df, val_df, test_df=None):
    if config.modality == 'text':
        tokenizer = AutoTokenizer.from_pretrained(config.model.text.name)
        train_dataset = TextDataset(train_df, config, tokenizer)
        val_dataset = TextDataset(val_df, config, tokenizer)
        collate_fn = None
    elif config.modality == 'audio':
        train_dataset = AudioDataset(train_df, config)
        val_dataset = AudioDataset(val_df, config)
        collate_fn = collate_audio
    else:
        raise ValueError(f"Modalità '{config.modality}' non supportata.")

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
    
    if test_df is not None:
        test_dataset = TextDataset(test_df, config, tokenizer) if config.modality == 'text' else AudioDataset(test_df, config)
        test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader, test_loader
        
    return train_loader, val_loader
===== ./src/config.py =====
import yaml
from pathlib import Path
from typing import Any

class Config:
    """
    Classe per caricare la configurazione da un file YAML e permettere
    l'accesso agli attributi tramite dot notation (es. config.data.audio_root).
    """
    def __init__(self, data: dict):
        for key, value in data.items():
            if isinstance(value, dict):
                setattr(self, key, Config(value))
            else:
                setattr(self, key, value)

    def __repr__(self):
        return str(self.__dict__)

    def to_dict(self) -> dict:
        result = {}
        for key, value in self.__dict__.items():
            if isinstance(value, Config):
                result[key] = value.to_dict()
            else:
                result[key] = value
        return result

def load_config(config_path: str | Path) -> Config:
    """Carica un file di configurazione YAML."""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"File di configurazione non trovato in: {path.resolve()}")
        
    with open(path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if not isinstance(config_data, dict):
        raise TypeError("La radice del file YAML deve essere un dizionario.")
        
    return Config(config_data)
===== ./src/utils.py =====
import torch
import numpy as np
import random
import gc
import torch.nn as nn
def set_seed(seed: int):
    """Imposta il seed per la riproducibilità."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def clear_memory():
    """Libera la memoria della GPU."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


===== ./src/engine.py =====
import torch
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, classification_report, confusion_matrix

from src.models import build_model
from src.utils import clear_memory

def train_epoch(model, loader, optimizer, scheduler, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in tqdm(loader, desc="Training", leave=False):
        optimizer.zero_grad()
        
        # --- FIX: QUESTA RIGA DEVE ESSERE ATTIVA ---
        # Sposta tutti i tensori del batch sul device corretto (es. GPU)
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        
        labels = batch.pop('labels')
        outputs = model(batch)
        
        loss = loss_fn(outputs, labels)
        loss.backward()
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def evaluate_epoch(model, loader, loss_fn, device, task, metric_name):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            # --- FIX: ANCHE QUESTA RIGA DEVE ESSERE ATTIVA ---
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            labels = batch.pop('labels')
            outputs = model(batch)
            
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            
            if task == 'classification':
                preds = torch.argmax(outputs, dim=1)
            else: # regression
                preds = outputs
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    
    if task == 'classification':
        if metric_name == 'accuracy':
            metric = accuracy_score(all_labels, all_preds)
        elif metric_name == 'f1':
            metric = f1_score(all_labels, all_preds, average='weighted')
        else:
            raise ValueError(f"Metrica '{metric_name}' non supportata per la classificazione.")
    else: # regression
        metric = root_mean_squared_error(all_labels, all_preds) # RMSE
        
    return avg_loss, metric

class Trainer:
    def __init__(self, config, train_loader, val_loader, fold):
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.fold = fold
        self.device = torch.device(config.device)
        self.output_path = Path(config.output_dir) / f"fold_{fold}"
        self.output_path.mkdir(parents=True, exist_ok=True)

    def train(self):
        clear_memory()
        model = build_model(self.config).to(self.device)
        if self.config.modality == 'text':
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        else: # audio
            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        
        num_training_steps = len(self.train_loader) * self.config.training.epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(num_training_steps * self.config.training.warmup_ratio), num_training_steps=num_training_steps)
        
        loss_fn = nn.CrossEntropyLoss() if self.config.task == 'classification' else nn.MSELoss()

        best_metric = -np.inf if self.config.training.eval_metric in ['accuracy', 'f1'] else np.inf
        patience_counter = 0

        print(f"--- Inizio Training Fold {self.fold} ---")
        for epoch in range(self.config.training.epochs):
            train_loss = train_epoch(model, self.train_loader, optimizer, scheduler, loss_fn, self.device)
            val_loss, val_metric = evaluate_epoch(model, self.val_loader, loss_fn, self.device, self.config.task, self.config.training.eval_metric)
            
            print(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val {self.config.training.eval_metric}: {val_metric:.4f}")

            is_better = (val_metric > best_metric) if self.config.training.eval_metric in ['accuracy', 'f1'] else (val_metric < best_metric)
            
            if is_better:
                best_metric = val_metric
                torch.save(model.state_dict(), self.output_path / "best_model.pt")
                print(f"-> Modello salvato. Nuova metrica migliore: {best_metric:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config.training.early_stopping_patience:
                print("-> Early stopping attivato.")
                break
        print(f"--- Fine Training Fold {self.fold} ---")

class Predictor:
    def __init__(self, config, test_loader):
        self.config = config
        self.test_loader = test_loader
        self.device = torch.device(config.device)
        self.output_dir = Path(config.output_dir)
    
    def predict(self):
        all_fold_scores = []
        
        for fold in range(self.config.k_folds):
            print(f"--- Inferenza con Fold {fold} ---")
            model_path = self.output_dir / f"fold_{fold}" / "best_model.pt"
            if not model_path.exists():
                print(f"ATTENZIONE: Modello per il fold {fold} non trovato in {model_path}. Salto.")
                continue

            model = build_model(self.config)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.to(self.device)
            model.eval()
            
            fold_scores = []
            test_ids = []
            
            with torch.no_grad():
                for batch in tqdm(self.test_loader, desc=f"Predicting Fold {fold}", leave=False):
                    ids = batch.pop("id")
                    test_ids.extend(ids)
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    outputs = model(batch)
                    
                    if self.config.task == 'classification':
                        scores = torch.softmax(outputs, dim=1)[:, 1] # Probabilità della classe positiva
                    else: # regression
                        scores = outputs
                    
                    fold_scores.extend(scores.cpu().numpy())
            
            all_fold_scores.append(fold_scores)
            
        # Ensembling: media degli score/probabilità
        mean_scores = np.mean(all_fold_scores, axis=0)
        
        # Crea DataFrame finale
        results_df = pd.DataFrame({'ID': test_ids[:len(mean_scores)], 'score': mean_scores})
        
        if self.config.task == 'classification':
            results_df['prediction'] = (results_df['score'] >= 0.5).astype(int)
        else:
            results_df['prediction'] = results_df['score']
            
        output_file = self.output_dir / "predictions.csv"
        results_df[['ID', 'prediction']].to_csv(output_file, index=False)
        print(f"\nPredizioni salvate in {output_file}")
        
class Evaluator:
    def __init__(self, config):
        self.config = config
        self.predictions_path = Path(config.output_dir) / "predictions.csv"
        if config.task == 'classification':
            self.labels_path = Path(config.data.test_task1_labels)
        else:
            self.labels_path = Path(config.data.test_task2_labels)
    
    def evaluate(self):
        if not self.predictions_path.exists():
            raise FileNotFoundError(f"File di predizioni non trovato: {self.predictions_path}")
        
        preds_df = pd.read_csv(self.predictions_path)
        labels_df = pd.read_csv(self.labels_path)
        
        # Assicuriamoci che la colonna ID si chiami allo stesso modo per il merge
        # A volte i CSV hanno nomi leggermente diversi (es. 'id', 'ID', 'adressfname')
        if 'ID' not in labels_df.columns:
             # Cerca una colonna che potrebbe essere l'ID
             possible_id_cols = [col for col in labels_df.columns if 'id' in col.lower() or 'name' in col.lower()]
             if possible_id_cols:
                 labels_df = labels_df.rename(columns={possible_id_cols[0]: 'ID'})
             else:
                 raise ValueError("Non riesco a trovare la colonna ID nel file delle etichette.")

        merged_df = pd.merge(preds_df, labels_df, on="ID")
        
        y_pred = merged_df['prediction']
        
        if self.config.task == 'classification':
            # --- FIX CRUCIALE: MAPPING DELLE ETICHETTE ---
            # Convertiamo le stringhe 'Control'/'ProbableAD' in 0/1
            # Adatta questo dizionario se le tue etichette sono diverse (es. 'CN'/'AD')
            label_mapping = {'Control': 0, 'ProbableAD': 1, 'CN': 0, 'AD': 1}
            
            # Usa la colonna corretta per la diagnosi. Potrebbe chiamarsi 'Dx', 'diagnosis', ecc.
            # Cerchiamo di essere flessibili.
            dx_col = None
            for col in ['Dx', 'diagnosis', 'label']:
                if col in merged_df.columns:
                    dx_col = col
                    break
            
            if dx_col is None:
                 raise ValueError(f"Colonna diagnosi non trovata. Colonne disponibili: {merged_df.columns}")

            # Applica il mapping. Se un valore non è nel dizionario, potrebbe dare errore o NaN.
            try:
                y_true = merged_df[dx_col].map(label_mapping).astype(int)
            except ValueError as e:
                 print(f"Errore durante il mapping delle etichette. Valori unici trovati in {dx_col}: {merged_df[dx_col].unique()}")
                 raise e
            # ---------------------------------------------

            print("----- Valutazione Classificazione -----")
            print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
            print(f"F1-Score (Weighted): {f1_score(y_true, y_pred, average='weighted'):.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true, y_pred, target_names=['CN', 'AD']))
            print("\nConfusion Matrix:")
            print(confusion_matrix(y_true, y_pred))
            print("------------------------------------")
        else: # regression
            y_true = merged_df['MMSE']
            print("----- Valutazione Regressione -----")
            rmse = root_mean_squared_error(y_true, y_pred)
            print(f"RMSE: {rmse:.4f}")
            print("------------------------------------")
===== ./src/__init__.py =====

===== ./scripts/transcribe.py =====
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import torch
import gc

# Aggiunge la root del progetto al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config

# ===================================================================
#      IMPLEMENTAZIONE CORRETTA PER NYRAHEALTH/CRISPERWHISPER
# ===================================================================
def get_crisperwhisper_transcriber(config):
    """
    Prepara la pipeline per CrisperWhisper.
    Carica il token dal file .env solo per questo engine.
    """
    # --- MODIFICA CHIAVE: IMPORT E CARICAMENTO LOCALE ---
    # Questo codice viene eseguito SOLO se l'engine è "crisperwhisper".
    # In questo modo, gli altri ambienti non hanno bisogno di 'python-dotenv'.
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("Info: File .env trovato e caricato per l'autenticazione a Hugging Face.")
    except ImportError:
        print("Info: Libreria 'python-dotenv' non trovata. Si procederà usando il token salvato da 'huggingface-cli login' se disponibile.")
    # --- FINE MODIFICA ---

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    except ImportError:
        print("\nERRORE: La libreria 'transformers' (versione custom) non è installata.")
        print("Assicurati di aver attivato 'crisper_env' e di aver eseguito 'pip install git+...'")
        sys.exit(1)

    cfg = config.transcription.crisperwhisper
    device = config.device
    torch_dtype = torch.float16 if "cuda" in device else torch.float32
    
    print(f"Caricamento backend CrisperWhisper (HF Pipeline) con modello '{cfg.model_id}'...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        cfg.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(cfg.model_id)
    transcription_pipeline = pipeline(
        "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor, chunk_length_s=30,
        batch_size=cfg.batch_size, torch_dtype=torch_dtype, device=device,
    )
    print("Backend CrisperWhisper inizializzato.")

    def transcribe_files(audio_dir, output_dir):
        # ... (Questa logica interna rimane invariata)
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                result = transcription_pipeline(str(audio_path))
                full_text = result["text"].strip()
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files

# ===================================================================
#             LOGICA PER NEMO E WHISPERX (INVARIATE)
# ===================================================================

def get_nemo_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import nemo.collections.asr as nemo_asr
    except ImportError:
        print("\nERRORE: NVIDIA NeMo Toolkit non è installato. Attiva l'ambiente 'nemo_env'.")
        sys.exit(1)
    cfg = config.transcription.nemo
    print(f"Caricamento backend NeMo con modello '{cfg.model_name}'...")
    backend = nemo_asr.models.ASRModel.from_pretrained(model_name=cfg.model_name)
    backend.to(torch.device(config.device))
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o sono già tutti presenti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        str_paths = [str(p) for p in audio_files]
        result = backend.transcribe(paths_to_audio_files=str_paths, batch_size=cfg.batch_size, channel_selector='average', verbose=False)
        transcriptions = result[0] if isinstance(result, tuple) else result
        for audio_path, text_obj in zip(audio_files, transcriptions):
            final_text = text_obj.text if hasattr(text_obj, 'text') else text_obj
            out_path = output_dir / f"{audio_path.stem}.txt"
            out_path.write_text(final_text.strip() if final_text else "", encoding='utf-8')
    return transcribe_files


def get_whisperx_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import whisperx
    except ImportError:
        print("\nERRORE: La libreria 'whisperx' non è installata. Attiva l'ambiente 'whisperx_env'.")
        sys.exit(1)
    cfg = config.transcription.whisperx
    print(f"Caricamento backend WhisperX con modello '{cfg.model_name}'...")
    model = whisperx.load_model(cfg.model_name, config.device, compute_type=cfg.compute_type, language=cfg.language)
    print("Backend WhisperX inizializzato.")
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                audio = whisperx.load_audio(str(audio_path))
                result = model.transcribe(audio, batch_size=cfg.batch_size)
                full_text = " ".join([segment['text'].strip() for segment in result["segments"]])
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files


def main():
    # L'import di dotenv è stato rimosso da qui.
    parser = argparse.ArgumentParser(description="Genera trascrizioni per i dati.")
    # ... (il resto della funzione main rimane identico)
    parser.add_argument("--config", type=str, required=True, help="Percorso al file config.yaml")
    args = parser.parse_args()
    config = load_config(args.config)
    engine = config.transcription.engine.lower()
    transcribe_function = None
    if engine == "crisperwhisper":
        transcribe_function = get_crisperwhisper_transcriber(config)
    elif engine == "nemo":
        transcribe_function = get_nemo_transcriber(config)
    elif engine == "whisperx":
        transcribe_function = get_whisperx_transcriber(config)
    else:
        print(f"ERRORE: Engine '{engine}' non supportato. Scegli tra 'crisperwhisper', 'nemo' o 'whisperx'.")
        sys.exit(1)
    model_folder_name = ""
    if engine == "crisperwhisper":
        model_folder_name = config.transcription.crisperwhisper.model_id.split('/')[-1]
    elif engine == "nemo":
        model_folder_name = config.transcription.nemo.model_name.split('/')[-1]
    elif engine == "whisperx":
        model_folder_name = f"WhisperX_{config.transcription.whisperx.model_name}"
    train_output_dir = Path(config.data.transcripts_root) / model_folder_name
    test_output_dir = Path(config.data.test_transcripts_root) / model_folder_name
    train_output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Gli output verranno salvati in: {train_output_dir} e {test_output_dir}")
    print(f"\n--- Inizio Trascrizione Training Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.audio_root), train_output_dir)
    print(f"\n--- Inizio Trascrizione Test Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.test_audio_root), test_output_dir)
    print("\nTrascrizione completata.")

if __name__ == "__main__":
    main()
===== ./scripts/run.py =====
import argparse
import sys
from pathlib import Path

# Aggiunge la root del progetto al path per permettere 'from src import ...'
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.utils import set_seed
from src.data import get_data_splits, get_dataloaders
from src.engine import Trainer, Predictor, Evaluator

def main():
    parser = argparse.ArgumentParser(description="Script principale per training, predizione e valutazione.")
    parser.add_argument("--config", type=str, required=True, help="Percorso al file di configurazione (es. config.yaml)")
    parser.add_argument("--mode", type=str, required=True, choices=["train", "predict", "evaluate"], help="Modalità di esecuzione.")
    args = parser.parse_args()

    config = load_config(args.config)
    set_seed(config.seed)

    if args.mode == 'train':
        print(f"Inizio training per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Gli output verranno salvati in: {config.output_dir}")
        
        # Il K-Fold è gestito qui
        for fold, train_df, val_df, _ in get_data_splits(config):
            train_loader, val_loader = get_dataloaders(config, train_df, val_df)
            trainer = Trainer(config, train_loader, val_loader, fold)
            trainer.train()
            
    elif args.mode == 'predict':
        print(f"Inizio predizione per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Uso i modelli da: {config.output_dir}")
        
        # Ottieni il test loader (usiamo un ciclo fittizio per prendere il primo test_df)
        for _, _, _, test_df in get_data_splits(config):
            _, _, test_loader = get_dataloaders(config, train_df=test_df, val_df=test_df, test_df=test_df)
            break # Usciamo dopo il primo, dato che test_df è sempre lo stesso
            
        predictor = Predictor(config, test_loader)
        predictor.predict()

    elif args.mode == 'evaluate':
        print(f"Inizio valutazione per il task '{config.task}'.")
        print(f"Valuto le predizioni in: {Path(config.output_dir) / 'predictions.csv'}")
        
        evaluator = Evaluator(config)
        evaluator.evaluate()

if __name__ == "__main__":
    main()
===== ./config.yaml =====
# ===================================================================
#                    CONFIGURAZIONE PRINCIPALE
# ===================================================================

# --- Impostazioni Generali ---
seed: 42
device: "cuda"  # "cuda" o "cpu"

# --- Percorsi dei Dati ---
data:
  audio_root: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio"
  transcripts_root: "transcripts"
  train_labels: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv"
  
  test_audio_root: "Adresso21/ADReSSo21-diagnosis-test/ADReSSo21/diagnosis/test-dist/audio"
  test_transcripts_root: "transcripts_test"
  test_task1_labels: "Adresso21/label_test_task1.csv"
  test_task2_labels: "Adresso21/label_test_task2.csv"

# ===================================================================
#                   CONFIGURAZIONE TRASCRIZIONE (ASR)
# ===================================================================
# Modifica 'engine' per scegliere quale motore ASR usare.
# Lo script leggerà solo la sezione di configurazione corrispondente.
# ===================================================================

transcription:
  # CAMBIA QUI: "crisperwhisper", "nemo", o "whisperx"
  engine: "whisperx"
  overwrite: false

  # --- SEZIONE CRISPERWHISPER RIVISTA E CORRETTA ---
  crisperwhisper:
    # L'unico parametro che ci serve è l'ID del modello su Hugging Face
    model_id: "nyrahealth/CrisperWhisper" 
    batch_size: 16
    compute_type: "float16" # o "int8"
    
  # --- Impostazioni per NeMo ---
  nemo:
    model_name: "nvidia/parakeet-tdt-0.6b-v2"
    batch_size: 16

  # --- Impostazioni per WhisperX ---
  whisperx:
    model_name: "nyrahealth/faster_CrisperWhisper"
    batch_size: 16
    compute_type: "float16"
    language: "en"
# ===================================================================
#                     CONFIGURAZIONE ESPERIMENTO
# ===================================================================
# Questa sezione controlla le operazioni di training, predizione e valutazione
# lanciate con 'python scripts/run.py'.
# ===================================================================

# --- Task e Modello ---
task: "classification"    # "classification" o "regression"
modality: "text"          # "text" o "audio"

# Modello di trascrizione da usare per l'addestramento del modello testuale.
# Deve corrispondere al nome della cartella creata da 'transcribe.py'.
# Esempio per Crisper: "Crisper_whisper-large-v3"
# Esempio per NeMo: "parakeet-tdt-0.6b-v2"
transcription_model_for_training: "WhisperX_nyrahealth/faster_CrisperWhisper"

# --- Cross-Validation ---
k_folds: 10
output_dir: "outputs/bert_classification_CLS_CrisperWhisper" # Cartella dove salvare modelli e predizioni

# --- Parametri del Modello ---
model:
  text:
    name: "bert-large-uncased"
    max_length: 275
    dropout: 0.1

  audio:
    name: "ecapa-tdnn"
    pretrained: "speechbrain/spkrec-ecapa-voxceleb"
    sample_rate: 16000
    trainable_encoder: true 
    dropout: 0.1

# --- Parametri di Addestramento ---
training:
  epochs: 15
  batch_size: 8
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  early_stopping_patience: 5
  eval_metric: "accuracy" # Per classificazione: "accuracy" o "f1". Per regressione: "rmse".
===== ./src/models.py =====
import torch
import torch.nn as nn
from transformers import AutoModel
from speechbrain.pretrained import EncoderClassifier
import sys
# ... (la classe BertClassifier rimane invariata) ...
class BertClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = AutoModel.from_pretrained(config.model.text.name)
        self.dropout = nn.Dropout(config.model.text.dropout)
        num_classes = 2 if config.task == 'classification' else 1
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, batch):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        if self.classifier.out_features > 1:
            return logits
        else:
            return logits.squeeze(-1)


class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        # --- FIX: Carica l'encoder direttamente sul device specificato in config.yaml ---
        # Questo è il metodo più sicuro per i modelli pre-addestrati complessi.
        self.encoder = EncoderClassifier.from_hparams(
            source=config.model.audio.pretrained,
            run_opts={"device": config.device}
        )
        
        if not config.model.audio.trainable_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = False

        # Ottiene la dimensione dell'embedding in modo sicuro
        with torch.no_grad():
            # Assicurati che anche il tensore fittizio sia sul device corretto
            dummy_input = torch.zeros(1, config.model.audio.sample_rate, device=config.device)
            dummy_embedding = self.encoder.encode_batch(dummy_input)
            num_features = dummy_embedding.shape[-1]
        
        num_classes = 2 if config.task == 'classification' else 1
        
        self.classifier_head = nn.Sequential(
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features, num_classes)
        )

    def forward(self, batch):
        waveforms = batch['waveform']
        
        is_trainable = any(p.requires_grad for p in self.encoder.parameters())
        with torch.set_grad_enabled(is_trainable):
            # Non è necessario passare 'lengths', encode_batch gestisce il padding
            embeddings = self.encoder.encode_batch(waveforms)

        embeddings = embeddings.squeeze(1) 
        outputs = self.classifier_head(embeddings)
        
        if self.classifier_head[-1].out_features > 1:
            return outputs
        else:
            return outputs.squeeze(-1)

def build_model(config):
    if config.modality == 'text':
        return BertClassifier(config)
    elif config.modality == 'audio':
        return EcapaTdnnClassifier(config)
    else:
        raise ValueError(f"Modello per la modalità '{config.modality}' non supportato.")
def bert_head_init(m: nn.Module, std: float = 0.02):
    if isinstance(m, nn.Linear):
        if hasattr(nn.init, "trunc_normal_"):
            nn.init.trunc_normal_(m.weight, mean=0.0, std=std, a=-2*std, b=2*std)
        else:
            # fallback: normal + clamp (~truncated)
            with torch.no_grad():
                m.weight.normal_(0.0, std).clamp_(-2*std, 2*std)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
===== ./src/data.py =====
from pathlib import Path
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from sklearn.model_selection import StratifiedKFold
import torchaudio
from torchaudio.functional import resample

class TextDataset(Dataset):
    # --- QUESTA CLASSE È CORRETTA E RIMANE INVARIATA ---
    def __init__(self, df: pd.DataFrame, config, tokenizer):
        self.df = df; self.config = config; self.tokenizer = tokenizer; self.is_test = 'label' not in df.columns
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        transcript_dir = Path(self.config.data.transcripts_root if not self.is_test else self.config.data.test_transcripts_root)
        transcript_path = transcript_dir / self.config.transcription_model_for_training / f"{file_id}.txt"
        try: text = transcript_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError: text = ""
        inputs = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.config.model.text.max_length,
            padding="max_length", truncation=True, return_attention_mask=True, return_tensors="pt",
        )
        item = {"input_ids": inputs["input_ids"].flatten(), "attention_mask": inputs["attention_mask"].flatten(), "id": file_id}
        if not self.is_test:
            item['labels'] = torch.tensor(row['label'], dtype=torch.long if self.config.task == 'classification' else torch.float32)
        return item

class AudioDataset(Dataset):
    def __init__(self, df: pd.DataFrame, config):
        self.df = df
        self.config = config
        self.is_test = 'label' not in df.columns
        self.audio_root = Path(self.config.data.audio_root if not self.is_test else self.config.data.test_audio_root)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        audio_path = self.audio_root / f"{file_id}.wav"
        if not audio_path.exists() and not self.is_test:
            group = 'ad' if row['label'] == 1 else 'cn'
            audio_path = self.audio_root / group / f"{file_id}.wav"

        waveform, sr = torchaudio.load(audio_path)
        
        # --- FIX #1: FORZARE L'AUDIO MONO ---
        # Se la waveform ha più di un canale (es. è stereo), ne facciamo la media
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        # ------------------------------------

        if sr != self.config.model.audio.sample_rate:
            waveform = resample(waveform, sr, self.config.model.audio.sample_rate)
        
        # Ora .squeeze() funzionerà sempre, perché l'input è sempre [1, n_samples]
        item = {"waveform": waveform.squeeze(0), "id": file_id}

        if not self.is_test:
            target = row['label']
            dtype = torch.long if self.config.task == 'classification' else torch.float32
            item['labels'] = torch.tensor(target, dtype=dtype)
            
        return item

def collate_audio(batch):
    waveforms = [item['waveform'] for item in batch]
    ids = [item['id'] for item in batch]
    
    # Eseguiamo il padding delle waveform
    padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)
    
    # --- FIX: Rimuoviamo il calcolo delle 'lengths' per semplicità ---
    collated_batch = {"waveform": padded_waveforms, "id": ids}
    
    if 'labels' in batch[0]:
        labels = torch.stack([item['labels'] for item in batch])
        collated_batch['labels'] = labels
        
    return collated_batch



def get_data_splits(config):
    """Prepara i DataFrame per training, validazione e test."""
    train_df = pd.read_csv(config.data.train_labels)
    # Rinominiamo la colonna ID in modo standard
    train_df = train_df.rename(columns={'adressfname': 'ID'})
    train_df = train_df.set_index('ID')
    
    # --- FIX CRUCIALE: LOGICA ROBUSTA PER TROVARE LA COLONNA DELLA DIAGNOSI ---
    diagnosis_col = None
    possible_cols = ['dx', 'diagnosis', 'Dx'] # Lista di nomi di colonna comuni per la diagnosi
    for col in possible_cols:
        if col in train_df.columns:
            diagnosis_col = col
            break
            
    if diagnosis_col is None:
        raise ValueError(f"Impossibile trovare la colonna della diagnosi. Colonne disponibili: {train_df.columns}")
        
    print(f"Info: Trovata colonna diagnosi '{diagnosis_col}'. Verrà usata per creare le etichette.")
    # --- FINE FIX ---

    if config.task == 'classification':
        # Usiamo la colonna trovata per creare le etichette
        train_df['label'] = train_df[diagnosis_col].apply(lambda x: 1 if x == 'ad' else 0)
    else:
        # Per la regressione, assumiamo che la colonna mmse esista
        if 'mmse' not in train_df.columns:
            raise ValueError(f"Task di regressione selezionato ma colonna 'mmse' non trovata. Colonne: {train_df.columns}")
        train_df['label'] = train_df['mmse']
        
    train_df = train_df.sample(frac=1, random_state=config.seed)
    
    test_df = pd.read_csv(config.data.test_task1_labels).rename(columns={'ID': 'id'}).set_index('id')

    skf = StratifiedKFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):
        train_split = train_df.iloc[train_idx]
        val_split = train_df.iloc[val_idx]
        yield fold, train_split, val_split, test_df
        
def get_dataloaders(config, train_df, val_df, test_df=None):
    if config.modality == 'text':
        tokenizer = AutoTokenizer.from_pretrained(config.model.text.name)
        train_dataset = TextDataset(train_df, config, tokenizer)
        val_dataset = TextDataset(val_df, config, tokenizer)
        collate_fn = None
    elif config.modality == 'audio':
        train_dataset = AudioDataset(train_df, config)
        val_dataset = AudioDataset(val_df, config)
        collate_fn = collate_audio
    else:
        raise ValueError(f"Modalità '{config.modality}' non supportata.")

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
    
    if test_df is not None:
        test_dataset = TextDataset(test_df, config, tokenizer) if config.modality == 'text' else AudioDataset(test_df, config)
        test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader, test_loader
        
    return train_loader, val_loader
===== ./src/config.py =====
import yaml
from pathlib import Path
from typing import Any

class Config:
    """
    Classe per caricare la configurazione da un file YAML e permettere
    l'accesso agli attributi tramite dot notation (es. config.data.audio_root).
    """
    def __init__(self, data: dict):
        for key, value in data.items():
            if isinstance(value, dict):
                setattr(self, key, Config(value))
            else:
                setattr(self, key, value)

    def __repr__(self):
        return str(self.__dict__)

    def to_dict(self) -> dict:
        result = {}
        for key, value in self.__dict__.items():
            if isinstance(value, Config):
                result[key] = value.to_dict()
            else:
                result[key] = value
        return result

def load_config(config_path: str | Path) -> Config:
    """Carica un file di configurazione YAML."""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"File di configurazione non trovato in: {path.resolve()}")
        
    with open(path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if not isinstance(config_data, dict):
        raise TypeError("La radice del file YAML deve essere un dizionario.")
        
    return Config(config_data)
===== ./src/utils.py =====
import torch
import numpy as np
import random
import gc
import torch.nn as nn
def set_seed(seed: int):
    """Imposta il seed per la riproducibilità."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def clear_memory():
    """Libera la memoria della GPU."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


===== ./src/engine.py =====
import torch
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, classification_report, confusion_matrix

from src.models import build_model
from src.utils import clear_memory

def train_epoch(model, loader, optimizer, scheduler, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in tqdm(loader, desc="Training", leave=False):
        optimizer.zero_grad()
        
        # --- FIX: QUESTA RIGA DEVE ESSERE ATTIVA ---
        # Sposta tutti i tensori del batch sul device corretto (es. GPU)
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        
        labels = batch.pop('labels')
        outputs = model(batch)
        
        loss = loss_fn(outputs, labels)
        loss.backward()
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def evaluate_epoch(model, loader, loss_fn, device, task, metric_name):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            # --- FIX: ANCHE QUESTA RIGA DEVE ESSERE ATTIVA ---
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            labels = batch.pop('labels')
            outputs = model(batch)
            
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            
            if task == 'classification':
                preds = torch.argmax(outputs, dim=1)
            else: # regression
                preds = outputs
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    
    if task == 'classification':
        if metric_name == 'accuracy':
            metric = accuracy_score(all_labels, all_preds)
        elif metric_name == 'f1':
            metric = f1_score(all_labels, all_preds, average='weighted')
        else:
            raise ValueError(f"Metrica '{metric_name}' non supportata per la classificazione.")
    else: # regression
        metric = root_mean_squared_error(all_labels, all_preds) # RMSE
        
    return avg_loss, metric

class Trainer:
    def __init__(self, config, train_loader, val_loader, fold):
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.fold = fold
        self.device = torch.device(config.device)
        self.output_path = Path(config.output_dir) / f"fold_{fold}"
        self.output_path.mkdir(parents=True, exist_ok=True)

    def train(self):
        clear_memory()
        model = build_model(self.config).to(self.device)
        if self.config.modality == 'text':
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        else: # audio
            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        
        num_training_steps = len(self.train_loader) * self.config.training.epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(num_training_steps * self.config.training.warmup_ratio), num_training_steps=num_training_steps)
        
        loss_fn = nn.CrossEntropyLoss() if self.config.task == 'classification' else nn.MSELoss()

        best_metric = -np.inf if self.config.training.eval_metric in ['accuracy', 'f1'] else np.inf
        patience_counter = 0

        print(f"--- Inizio Training Fold {self.fold} ---")
        for epoch in range(self.config.training.epochs):
            train_loss = train_epoch(model, self.train_loader, optimizer, scheduler, loss_fn, self.device)
            val_loss, val_metric = evaluate_epoch(model, self.val_loader, loss_fn, self.device, self.config.task, self.config.training.eval_metric)
            
            print(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val {self.config.training.eval_metric}: {val_metric:.4f}")

            is_better = (val_metric > best_metric) if self.config.training.eval_metric in ['accuracy', 'f1'] else (val_metric < best_metric)
            
            if is_better:
                best_metric = val_metric
                torch.save(model.state_dict(), self.output_path / "best_model.pt")
                print(f"-> Modello salvato. Nuova metrica migliore: {best_metric:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config.training.early_stopping_patience:
                print("-> Early stopping attivato.")
                break
        print(f"--- Fine Training Fold {self.fold} ---")

class Predictor:
    def __init__(self, config, test_loader):
        self.config = config
        self.test_loader = test_loader
        self.device = torch.device(config.device)
        self.output_dir = Path(config.output_dir)
    
    def predict(self):
        all_fold_scores = []
        
        for fold in range(self.config.k_folds):
            print(f"--- Inferenza con Fold {fold} ---")
            model_path = self.output_dir / f"fold_{fold}" / "best_model.pt"
            if not model_path.exists():
                print(f"ATTENZIONE: Modello per il fold {fold} non trovato in {model_path}. Salto.")
                continue

            model = build_model(self.config)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.to(self.device)
            model.eval()
            
            fold_scores = []
            test_ids = []
            
            with torch.no_grad():
                for batch in tqdm(self.test_loader, desc=f"Predicting Fold {fold}", leave=False):
                    ids = batch.pop("id")
                    test_ids.extend(ids)
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    outputs = model(batch)
                    
                    if self.config.task == 'classification':
                        scores = torch.softmax(outputs, dim=1)[:, 1] # Probabilità della classe positiva
                    else: # regression
                        scores = outputs
                    
                    fold_scores.extend(scores.cpu().numpy())
            
            all_fold_scores.append(fold_scores)
            
        # Ensembling: media degli score/probabilità
        mean_scores = np.mean(all_fold_scores, axis=0)
        
        # Crea DataFrame finale
        results_df = pd.DataFrame({'ID': test_ids[:len(mean_scores)], 'score': mean_scores})
        
        if self.config.task == 'classification':
            results_df['prediction'] = (results_df['score'] >= 0.5).astype(int)
        else:
            results_df['prediction'] = results_df['score']
            
        output_file = self.output_dir / "predictions.csv"
        results_df[['ID', 'prediction']].to_csv(output_file, index=False)
        print(f"\nPredizioni salvate in {output_file}")
        
class Evaluator:
    def __init__(self, config):
        self.config = config
        self.predictions_path = Path(config.output_dir) / "predictions.csv"
        if config.task == 'classification':
            self.labels_path = Path(config.data.test_task1_labels)
        else:
            self.labels_path = Path(config.data.test_task2_labels)
    
    def evaluate(self):
        if not self.predictions_path.exists():
            raise FileNotFoundError(f"File di predizioni non trovato: {self.predictions_path}")
        
        preds_df = pd.read_csv(self.predictions_path)
        labels_df = pd.read_csv(self.labels_path)
        
        # Assicuriamoci che la colonna ID si chiami allo stesso modo per il merge
        # A volte i CSV hanno nomi leggermente diversi (es. 'id', 'ID', 'adressfname')
        if 'ID' not in labels_df.columns:
             # Cerca una colonna che potrebbe essere l'ID
             possible_id_cols = [col for col in labels_df.columns if 'id' in col.lower() or 'name' in col.lower()]
             if possible_id_cols:
                 labels_df = labels_df.rename(columns={possible_id_cols[0]: 'ID'})
             else:
                 raise ValueError("Non riesco a trovare la colonna ID nel file delle etichette.")

        merged_df = pd.merge(preds_df, labels_df, on="ID")
        
        y_pred = merged_df['prediction']
        
        if self.config.task == 'classification':
            # --- FIX CRUCIALE: MAPPING DELLE ETICHETTE ---
            # Convertiamo le stringhe 'Control'/'ProbableAD' in 0/1
            # Adatta questo dizionario se le tue etichette sono diverse (es. 'CN'/'AD')
            label_mapping = {'Control': 0, 'ProbableAD': 1, 'CN': 0, 'AD': 1}
            
            # Usa la colonna corretta per la diagnosi. Potrebbe chiamarsi 'Dx', 'diagnosis', ecc.
            # Cerchiamo di essere flessibili.
            dx_col = None
            for col in ['Dx', 'diagnosis', 'label']:
                if col in merged_df.columns:
                    dx_col = col
                    break
            
            if dx_col is None:
                 raise ValueError(f"Colonna diagnosi non trovata. Colonne disponibili: {merged_df.columns}")

            # Applica il mapping. Se un valore non è nel dizionario, potrebbe dare errore o NaN.
            try:
                y_true = merged_df[dx_col].map(label_mapping).astype(int)
            except ValueError as e:
                 print(f"Errore durante il mapping delle etichette. Valori unici trovati in {dx_col}: {merged_df[dx_col].unique()}")
                 raise e
            # ---------------------------------------------

            print("----- Valutazione Classificazione -----")
            print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
            print(f"F1-Score (Weighted): {f1_score(y_true, y_pred, average='weighted'):.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true, y_pred, target_names=['CN', 'AD']))
            print("\nConfusion Matrix:")
            print(confusion_matrix(y_true, y_pred))
            print("------------------------------------")
        else: # regression
            y_true = merged_df['MMSE']
            print("----- Valutazione Regressione -----")
            rmse = root_mean_squared_error(y_true, y_pred)
            print(f"RMSE: {rmse:.4f}")
            print("------------------------------------")
===== ./src/__init__.py =====

===== ./scripts/transcribe.py =====
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import torch
import gc

# Aggiunge la root del progetto al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config

# ===================================================================
#      IMPLEMENTAZIONE CORRETTA PER NYRAHEALTH/CRISPERWHISPER
# ===================================================================
def get_crisperwhisper_transcriber(config):
    """
    Prepara la pipeline per CrisperWhisper.
    Carica il token dal file .env solo per questo engine.
    """
    # --- MODIFICA CHIAVE: IMPORT E CARICAMENTO LOCALE ---
    # Questo codice viene eseguito SOLO se l'engine è "crisperwhisper".
    # In questo modo, gli altri ambienti non hanno bisogno di 'python-dotenv'.
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("Info: File .env trovato e caricato per l'autenticazione a Hugging Face.")
    except ImportError:
        print("Info: Libreria 'python-dotenv' non trovata. Si procederà usando il token salvato da 'huggingface-cli login' se disponibile.")
    # --- FINE MODIFICA ---

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    except ImportError:
        print("\nERRORE: La libreria 'transformers' (versione custom) non è installata.")
        print("Assicurati di aver attivato 'crisper_env' e di aver eseguito 'pip install git+...'")
        sys.exit(1)

    cfg = config.transcription.crisperwhisper
    device = config.device
    torch_dtype = torch.float16 if "cuda" in device else torch.float32
    
    print(f"Caricamento backend CrisperWhisper (HF Pipeline) con modello '{cfg.model_id}'...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        cfg.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(cfg.model_id)
    transcription_pipeline = pipeline(
        "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor, chunk_length_s=30,
        batch_size=cfg.batch_size, torch_dtype=torch_dtype, device=device,
    )
    print("Backend CrisperWhisper inizializzato.")

    def transcribe_files(audio_dir, output_dir):
        # ... (Questa logica interna rimane invariata)
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                result = transcription_pipeline(str(audio_path))
                full_text = result["text"].strip()
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files

# ===================================================================
#             LOGICA PER NEMO E WHISPERX (INVARIATE)
# ===================================================================

def get_nemo_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import nemo.collections.asr as nemo_asr
    except ImportError:
        print("\nERRORE: NVIDIA NeMo Toolkit non è installato. Attiva l'ambiente 'nemo_env'.")
        sys.exit(1)
    cfg = config.transcription.nemo
    print(f"Caricamento backend NeMo con modello '{cfg.model_name}'...")
    backend = nemo_asr.models.ASRModel.from_pretrained(model_name=cfg.model_name)
    backend.to(torch.device(config.device))
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o sono già tutti presenti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        str_paths = [str(p) for p in audio_files]
        result = backend.transcribe(paths_to_audio_files=str_paths, batch_size=cfg.batch_size, channel_selector='average', verbose=False)
        transcriptions = result[0] if isinstance(result, tuple) else result
        for audio_path, text_obj in zip(audio_files, transcriptions):
            final_text = text_obj.text if hasattr(text_obj, 'text') else text_obj
            out_path = output_dir / f"{audio_path.stem}.txt"
            out_path.write_text(final_text.strip() if final_text else "", encoding='utf-8')
    return transcribe_files


def get_whisperx_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import whisperx
    except ImportError:
        print("\nERRORE: La libreria 'whisperx' non è installata. Attiva l'ambiente 'whisperx_env'.")
        sys.exit(1)
    cfg = config.transcription.whisperx
    print(f"Caricamento backend WhisperX con modello '{cfg.model_name}'...")
    model = whisperx.load_model(cfg.model_name, config.device, compute_type=cfg.compute_type, language=cfg.language)
    print("Backend WhisperX inizializzato.")
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                audio = whisperx.load_audio(str(audio_path))
                result = model.transcribe(audio, batch_size=cfg.batch_size)
                full_text = " ".join([segment['text'].strip() for segment in result["segments"]])
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files


def main():
    # L'import di dotenv è stato rimosso da qui.
    parser = argparse.ArgumentParser(description="Genera trascrizioni per i dati.")
    # ... (il resto della funzione main rimane identico)
    parser.add_argument("--config", type=str, required=True, help="Percorso al file config.yaml")
    args = parser.parse_args()
    config = load_config(args.config)
    engine = config.transcription.engine.lower()
    transcribe_function = None
    if engine == "crisperwhisper":
        transcribe_function = get_crisperwhisper_transcriber(config)
    elif engine == "nemo":
        transcribe_function = get_nemo_transcriber(config)
    elif engine == "whisperx":
        transcribe_function = get_whisperx_transcriber(config)
    else:
        print(f"ERRORE: Engine '{engine}' non supportato. Scegli tra 'crisperwhisper', 'nemo' o 'whisperx'.")
        sys.exit(1)
    model_folder_name = ""
    if engine == "crisperwhisper":
        model_folder_name = config.transcription.crisperwhisper.model_id.split('/')[-1]
    elif engine == "nemo":
        model_folder_name = config.transcription.nemo.model_name.split('/')[-1]
    elif engine == "whisperx":
        model_folder_name = f"WhisperX_{config.transcription.whisperx.model_name}"
    train_output_dir = Path(config.data.transcripts_root) / model_folder_name
    test_output_dir = Path(config.data.test_transcripts_root) / model_folder_name
    train_output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Gli output verranno salvati in: {train_output_dir} e {test_output_dir}")
    print(f"\n--- Inizio Trascrizione Training Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.audio_root), train_output_dir)
    print(f"\n--- Inizio Trascrizione Test Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.test_audio_root), test_output_dir)
    print("\nTrascrizione completata.")

if __name__ == "__main__":
    main()
===== ./scripts/run.py =====
import argparse
import sys
from pathlib import Path

# Aggiunge la root del progetto al path per permettere 'from src import ...'
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.utils import set_seed
from src.data import get_data_splits, get_dataloaders
from src.engine import Trainer, Predictor, Evaluator

def main():
    parser = argparse.ArgumentParser(description="Script principale per training, predizione e valutazione.")
    parser.add_argument("--config", type=str, required=True, help="Percorso al file di configurazione (es. config.yaml)")
    parser.add_argument("--mode", type=str, required=True, choices=["train", "predict", "evaluate"], help="Modalità di esecuzione.")
    args = parser.parse_args()

    config = load_config(args.config)
    set_seed(config.seed)

    if args.mode == 'train':
        print(f"Inizio training per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Gli output verranno salvati in: {config.output_dir}")
        
        # Il K-Fold è gestito qui
        for fold, train_df, val_df, _ in get_data_splits(config):
            train_loader, val_loader = get_dataloaders(config, train_df, val_df)
            trainer = Trainer(config, train_loader, val_loader, fold)
            trainer.train()
            
    elif args.mode == 'predict':
        print(f"Inizio predizione per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Uso i modelli da: {config.output_dir}")
        
        # Ottieni il test loader (usiamo un ciclo fittizio per prendere il primo test_df)
        for _, _, _, test_df in get_data_splits(config):
            _, _, test_loader = get_dataloaders(config, train_df=test_df, val_df=test_df, test_df=test_df)
            break # Usciamo dopo il primo, dato che test_df è sempre lo stesso
            
        predictor = Predictor(config, test_loader)
        predictor.predict()

    elif args.mode == 'evaluate':
        print(f"Inizio valutazione per il task '{config.task}'.")
        print(f"Valuto le predizioni in: {Path(config.output_dir) / 'predictions.csv'}")
        
        evaluator = Evaluator(config)
        evaluator.evaluate()

if __name__ == "__main__":
    main()
===== ./config.yaml =====
# ===================================================================
#                    CONFIGURAZIONE PRINCIPALE
# ===================================================================

# --- Impostazioni Generali ---
seed: 42
device: "cuda"  # "cuda" o "cpu"

# --- Percorsi dei Dati ---
data:
  audio_root: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio"
  transcripts_root: "transcripts"
  train_labels: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv"
  
  test_audio_root: "Adresso21/ADReSSo21-diagnosis-test/ADReSSo21/diagnosis/test-dist/audio"
  test_transcripts_root: "transcripts_test"
  test_task1_labels: "Adresso21/label_test_task1.csv"
  test_task2_labels: "Adresso21/label_test_task2.csv"

# ===================================================================
#                   CONFIGURAZIONE TRASCRIZIONE (ASR)
# ===================================================================
# Modifica 'engine' per scegliere quale motore ASR usare.
# Lo script leggerà solo la sezione di configurazione corrispondente.
# ===================================================================

transcription:
  # CAMBIA QUI: "crisperwhisper", "nemo", o "whisperx"
  engine: "whisperx"
  overwrite: false

  # --- SEZIONE CRISPERWHISPER RIVISTA E CORRETTA ---
  crisperwhisper:
    # L'unico parametro che ci serve è l'ID del modello su Hugging Face
    model_id: "nyrahealth/CrisperWhisper" 
    batch_size: 16
    compute_type: "float16" # o "int8"
    
  # --- Impostazioni per NeMo ---
  nemo:
    model_name: "nvidia/parakeet-tdt-0.6b-v2"
    batch_size: 16

  # --- Impostazioni per WhisperX ---
  whisperx:
    model_name: "nyrahealth/faster_CrisperWhisper"
    batch_size: 16
    compute_type: "float16"
    language: "en"
# ===================================================================
#                     CONFIGURAZIONE ESPERIMENTO
# ===================================================================
# Questa sezione controlla le operazioni di training, predizione e valutazione
# lanciate con 'python scripts/run.py'.
# ===================================================================

# --- Task e Modello ---
task: "classification"    # "classification" o "regression"
modality: "text"          # "text" o "audio"

# Modello di trascrizione da usare per l'addestramento del modello testuale.
# Deve corrispondere al nome della cartella creata da 'transcribe.py'.
# Esempio per Crisper: "Crisper_whisper-large-v3"
# Esempio per NeMo: "parakeet-tdt-0.6b-v2"
transcription_model_for_training: "CrisperWhisper"

# --- Cross-Validation ---
k_folds: 10
output_dir: "outputs/bert_classification_crisper" # Cartella dove salvare modelli e predizioni

# --- Parametri del Modello ---
model:
  text:
    name: "bert-large-uncased"
    max_length: 275
    dropout: 0.1

  audio:
    name: "ecapa-tdnn"
    pretrained: "speechbrain/spkrec-ecapa-voxceleb"
    sample_rate: 16000
    trainable_encoder: true 
    dropout: 0.1

# --- Parametri di Addestramento ---
training:
  epochs: 15
  batch_size: 8
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  early_stopping_patience: 5
  eval_metric: "accuracy" # Per classificazione: "accuracy" o "f1". Per regressione: "rmse".
===== ./src/models.py =====
import torch
import torch.nn as nn
from transformers import AutoModel
from speechbrain.pretrained import EncoderClassifier
import sys
# ... (la classe BertClassifier rimane invariata) ...
class BertClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = AutoModel.from_pretrained(config.model.text.name)
        self.dropout = nn.Dropout(config.model.text.dropout)
        num_classes = 2 if config.task == 'classification' else 1
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
    def forward(self, batch):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        if self.classifier.out_features > 1:
            return logits
        else:
            return logits.squeeze(-1)


class EcapaTdnnClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        # --- FIX: Carica l'encoder direttamente sul device specificato in config.yaml ---
        # Questo è il metodo più sicuro per i modelli pre-addestrati complessi.
        self.encoder = EncoderClassifier.from_hparams(
            source=config.model.audio.pretrained,
            run_opts={"device": config.device}
        )
        
        if not config.model.audio.trainable_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = False

        # Ottiene la dimensione dell'embedding in modo sicuro
        with torch.no_grad():
            # Assicurati che anche il tensore fittizio sia sul device corretto
            dummy_input = torch.zeros(1, config.model.audio.sample_rate, device=config.device)
            dummy_embedding = self.encoder.encode_batch(dummy_input)
            num_features = dummy_embedding.shape[-1]
        
        num_classes = 2 if config.task == 'classification' else 1
        
        self.classifier_head = nn.Sequential(
            nn.Dropout(config.model.audio.dropout),
            nn.Linear(num_features, num_classes)
        )

    def forward(self, batch):
        waveforms = batch['waveform']
        
        is_trainable = any(p.requires_grad for p in self.encoder.parameters())
        with torch.set_grad_enabled(is_trainable):
            # Non è necessario passare 'lengths', encode_batch gestisce il padding
            embeddings = self.encoder.encode_batch(waveforms)

        embeddings = embeddings.squeeze(1) 
        outputs = self.classifier_head(embeddings)
        
        if self.classifier_head[-1].out_features > 1:
            return outputs
        else:
            return outputs.squeeze(-1)

def build_model(config):
    if config.modality == 'text':
        return BertClassifier(config)
    elif config.modality == 'audio':
        return EcapaTdnnClassifier(config)
    else:
        raise ValueError(f"Modello per la modalità '{config.modality}' non supportato.")

===== ./src/data.py =====
from pathlib import Path
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoTokenizer
from sklearn.model_selection import StratifiedKFold
import torchaudio
from torchaudio.functional import resample

class TextDataset(Dataset):
    # --- QUESTA CLASSE È CORRETTA E RIMANE INVARIATA ---
    def __init__(self, df: pd.DataFrame, config, tokenizer):
        self.df = df; self.config = config; self.tokenizer = tokenizer; self.is_test = 'label' not in df.columns
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        transcript_dir = Path(self.config.data.transcripts_root if not self.is_test else self.config.data.test_transcripts_root)
        transcript_path = transcript_dir / self.config.transcription_model_for_training / f"{file_id}.txt"
        try: text = transcript_path.read_text(encoding="utf-8").strip()
        except FileNotFoundError: text = ""
        inputs = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.config.model.text.max_length,
            padding="max_length", truncation=True, return_attention_mask=True, return_tensors="pt",
        )
        item = {"input_ids": inputs["input_ids"].flatten(), "attention_mask": inputs["attention_mask"].flatten(), "id": file_id}
        if not self.is_test:
            item['labels'] = torch.tensor(row['label'], dtype=torch.long if self.config.task == 'classification' else torch.float32)
        return item

class AudioDataset(Dataset):
    def __init__(self, df: pd.DataFrame, config):
        self.df = df
        self.config = config
        self.is_test = 'label' not in df.columns
        self.audio_root = Path(self.config.data.audio_root if not self.is_test else self.config.data.test_audio_root)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_id = row.name
        audio_path = self.audio_root / f"{file_id}.wav"
        if not audio_path.exists() and not self.is_test:
            group = 'ad' if row['label'] == 1 else 'cn'
            audio_path = self.audio_root / group / f"{file_id}.wav"

        waveform, sr = torchaudio.load(audio_path)
        
        # --- FIX #1: FORZARE L'AUDIO MONO ---
        # Se la waveform ha più di un canale (es. è stereo), ne facciamo la media
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        # ------------------------------------

        if sr != self.config.model.audio.sample_rate:
            waveform = resample(waveform, sr, self.config.model.audio.sample_rate)
        
        # Ora .squeeze() funzionerà sempre, perché l'input è sempre [1, n_samples]
        item = {"waveform": waveform.squeeze(0), "id": file_id}

        if not self.is_test:
            target = row['label']
            dtype = torch.long if self.config.task == 'classification' else torch.float32
            item['labels'] = torch.tensor(target, dtype=dtype)
            
        return item

def collate_audio(batch):
    waveforms = [item['waveform'] for item in batch]
    ids = [item['id'] for item in batch]
    
    # Eseguiamo il padding delle waveform
    padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)
    
    # --- FIX: Rimuoviamo il calcolo delle 'lengths' per semplicità ---
    collated_batch = {"waveform": padded_waveforms, "id": ids}
    
    if 'labels' in batch[0]:
        labels = torch.stack([item['labels'] for item in batch])
        collated_batch['labels'] = labels
        
    return collated_batch



def get_data_splits(config):
    """Prepara i DataFrame per training, validazione e test."""
    train_df = pd.read_csv(config.data.train_labels)
    # Rinominiamo la colonna ID in modo standard
    train_df = train_df.rename(columns={'adressfname': 'ID'})
    train_df = train_df.set_index('ID')
    
    # --- FIX CRUCIALE: LOGICA ROBUSTA PER TROVARE LA COLONNA DELLA DIAGNOSI ---
    diagnosis_col = None
    possible_cols = ['dx', 'diagnosis', 'Dx'] # Lista di nomi di colonna comuni per la diagnosi
    for col in possible_cols:
        if col in train_df.columns:
            diagnosis_col = col
            break
            
    if diagnosis_col is None:
        raise ValueError(f"Impossibile trovare la colonna della diagnosi. Colonne disponibili: {train_df.columns}")
        
    print(f"Info: Trovata colonna diagnosi '{diagnosis_col}'. Verrà usata per creare le etichette.")
    # --- FINE FIX ---

    if config.task == 'classification':
        # Usiamo la colonna trovata per creare le etichette
        train_df['label'] = train_df[diagnosis_col].apply(lambda x: 1 if x == 'ad' else 0)
    else:
        # Per la regressione, assumiamo che la colonna mmse esista
        if 'mmse' not in train_df.columns:
            raise ValueError(f"Task di regressione selezionato ma colonna 'mmse' non trovata. Colonne: {train_df.columns}")
        train_df['label'] = train_df['mmse']
        
    train_df = train_df.sample(frac=1, random_state=config.seed)
    
    test_df = pd.read_csv(config.data.test_task1_labels).rename(columns={'ID': 'id'}).set_index('id')

    skf = StratifiedKFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):
        train_split = train_df.iloc[train_idx]
        val_split = train_df.iloc[val_idx]
        yield fold, train_split, val_split, test_df
        
def get_dataloaders(config, train_df, val_df, test_df=None):
    if config.modality == 'text':
        tokenizer = AutoTokenizer.from_pretrained(config.model.text.name)
        train_dataset = TextDataset(train_df, config, tokenizer)
        val_dataset = TextDataset(val_df, config, tokenizer)
        collate_fn = None
    elif config.modality == 'audio':
        train_dataset = AudioDataset(train_df, config)
        val_dataset = AudioDataset(val_df, config)
        collate_fn = collate_audio
    else:
        raise ValueError(f"Modalità '{config.modality}' non supportata.")

    train_loader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
    
    if test_df is not None:
        test_dataset = TextDataset(test_df, config, tokenizer) if config.modality == 'text' else AudioDataset(test_df, config)
        test_loader = DataLoader(test_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader, test_loader
        
    return train_loader, val_loader
===== ./src/config.py =====
import yaml
from pathlib import Path
from typing import Any

class Config:
    """
    Classe per caricare la configurazione da un file YAML e permettere
    l'accesso agli attributi tramite dot notation (es. config.data.audio_root).
    """
    def __init__(self, data: dict):
        for key, value in data.items():
            if isinstance(value, dict):
                setattr(self, key, Config(value))
            else:
                setattr(self, key, value)

    def __repr__(self):
        return str(self.__dict__)

    def to_dict(self) -> dict:
        result = {}
        for key, value in self.__dict__.items():
            if isinstance(value, Config):
                result[key] = value.to_dict()
            else:
                result[key] = value
        return result

def load_config(config_path: str | Path) -> Config:
    """Carica un file di configurazione YAML."""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"File di configurazione non trovato in: {path.resolve()}")
        
    with open(path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if not isinstance(config_data, dict):
        raise TypeError("La radice del file YAML deve essere un dizionario.")
        
    return Config(config_data)
===== ./src/utils.py =====
import torch
import numpy as np
import random
import gc
import torch.nn as nn
def set_seed(seed: int):
    """Imposta il seed per la riproducibilità."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def clear_memory():
    """Libera la memoria della GPU."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


===== ./src/engine.py =====
import torch
import torch.nn as nn
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, classification_report, confusion_matrix

from src.models import build_model
from src.utils import clear_memory

def train_epoch(model, loader, optimizer, scheduler, loss_fn, device):
    model.train()
    total_loss = 0
    for batch in tqdm(loader, desc="Training", leave=False):
        optimizer.zero_grad()
        
        # --- FIX: QUESTA RIGA DEVE ESSERE ATTIVA ---
        # Sposta tutti i tensori del batch sul device corretto (es. GPU)
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
        
        labels = batch.pop('labels')
        outputs = model(batch)
        
        loss = loss_fn(outputs, labels)
        loss.backward()
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def evaluate_epoch(model, loader, loss_fn, device, task, metric_name):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            # --- FIX: ANCHE QUESTA RIGA DEVE ESSERE ATTIVA ---
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            labels = batch.pop('labels')
            print(labels)
            outputs = model(batch)
            print(batch,outputs)
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            
            if task == 'classification':
                preds = torch.argmax(outputs, dim=1)
            else: # regression
                preds = outputs
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    
    if task == 'classification':
        if metric_name == 'accuracy':
            metric = accuracy_score(all_labels, all_preds)
        elif metric_name == 'f1':
            metric = f1_score(all_labels, all_preds, average='weighted')
        else:
            raise ValueError(f"Metrica '{metric_name}' non supportata per la classificazione.")
    else: # regression
        metric = root_mean_squared_error(all_labels, all_preds) # RMSE
        
    return avg_loss, metric

class Trainer:
    def __init__(self, config, train_loader, val_loader, fold):
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.fold = fold
        self.device = torch.device(config.device)
        self.output_path = Path(config.output_dir) / f"fold_{fold}"
        self.output_path.mkdir(parents=True, exist_ok=True)

    def train(self):
        clear_memory()
        model = build_model(self.config).to(self.device)
        if self.config.modality == 'text':
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        else: # audio
            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.training.learning_rate, weight_decay=self.config.training.weight_decay)
        
        num_training_steps = len(self.train_loader) * self.config.training.epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(num_training_steps * self.config.training.warmup_ratio), num_training_steps=num_training_steps)
        
        loss_fn = nn.CrossEntropyLoss() if self.config.task == 'classification' else nn.MSELoss()

        best_metric = -np.inf if self.config.training.eval_metric in ['accuracy', 'f1'] else np.inf
        patience_counter = 0

        print(f"--- Inizio Training Fold {self.fold} ---")
        for epoch in range(self.config.training.epochs):
            train_loss = train_epoch(model, self.train_loader, optimizer, scheduler, loss_fn, self.device)
            val_loss, val_metric = evaluate_epoch(model, self.val_loader, loss_fn, self.device, self.config.task, self.config.training.eval_metric)
            
            print(f"Epoch {epoch+1}/{self.config.training.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val {self.config.training.eval_metric}: {val_metric:.4f}")

            is_better = (val_metric > best_metric) if self.config.training.eval_metric in ['accuracy', 'f1'] else (val_metric < best_metric)
            
            if is_better:
                best_metric = val_metric
                torch.save(model.state_dict(), self.output_path / "best_model.pt")
                print(f"-> Modello salvato. Nuova metrica migliore: {best_metric:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config.training.early_stopping_patience:
                print("-> Early stopping attivato.")
                break
        print(f"--- Fine Training Fold {self.fold} ---")

class Predictor:
    def __init__(self, config, test_loader):
        self.config = config
        self.test_loader = test_loader
        self.device = torch.device(config.device)
        self.output_dir = Path(config.output_dir)
    
    def predict(self):
        all_fold_scores = []
        
        for fold in range(self.config.k_folds):
            print(f"--- Inferenza con Fold {fold} ---")
            model_path = self.output_dir / f"fold_{fold}" / "best_model.pt"
            if not model_path.exists():
                print(f"ATTENZIONE: Modello per il fold {fold} non trovato in {model_path}. Salto.")
                continue

            model = build_model(self.config)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.to(self.device)
            model.eval()
            
            fold_scores = []
            test_ids = []
            
            with torch.no_grad():
                for batch in tqdm(self.test_loader, desc=f"Predicting Fold {fold}", leave=False):
                    ids = batch.pop("id")
                    test_ids.extend(ids)
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    
                    outputs = model(batch)
                    
                    if self.config.task == 'classification':
                        scores = torch.softmax(outputs, dim=1)[:, 1] # Probabilità della classe positiva
                    else: # regression
                        scores = outputs
                    
                    fold_scores.extend(scores.cpu().numpy())
            
            all_fold_scores.append(fold_scores)
            
        # Ensembling: media degli score/probabilità
        mean_scores = np.mean(all_fold_scores, axis=0)
        
        # Crea DataFrame finale
        results_df = pd.DataFrame({'ID': test_ids[:len(mean_scores)], 'score': mean_scores})
        
        if self.config.task == 'classification':
            results_df['prediction'] = (results_df['score'] >= 0.5).astype(int)
        else:
            results_df['prediction'] = results_df['score']
            
        output_file = self.output_dir / "predictions.csv"
        results_df[['ID', 'prediction']].to_csv(output_file, index=False)
        print(f"\nPredizioni salvate in {output_file}")
        
class Evaluator:
    def __init__(self, config):
        self.config = config
        self.predictions_path = Path(config.output_dir) / "predictions.csv"
        if config.task == 'classification':
            self.labels_path = Path(config.data.test_task1_labels)
        else:
            self.labels_path = Path(config.data.test_task2_labels)
    
    def evaluate(self):
        if not self.predictions_path.exists():
            raise FileNotFoundError(f"File di predizioni non trovato: {self.predictions_path}")
        
        preds_df = pd.read_csv(self.predictions_path)
        labels_df = pd.read_csv(self.labels_path)
        
        # Assicuriamoci che la colonna ID si chiami allo stesso modo per il merge
        # A volte i CSV hanno nomi leggermente diversi (es. 'id', 'ID', 'adressfname')
        if 'ID' not in labels_df.columns:
             # Cerca una colonna che potrebbe essere l'ID
             possible_id_cols = [col for col in labels_df.columns if 'id' in col.lower() or 'name' in col.lower()]
             if possible_id_cols:
                 labels_df = labels_df.rename(columns={possible_id_cols[0]: 'ID'})
             else:
                 raise ValueError("Non riesco a trovare la colonna ID nel file delle etichette.")

        merged_df = pd.merge(preds_df, labels_df, on="ID")
        
        y_pred = merged_df['prediction']
        
        if self.config.task == 'classification':
            # --- FIX CRUCIALE: MAPPING DELLE ETICHETTE ---
            # Convertiamo le stringhe 'Control'/'ProbableAD' in 0/1
            # Adatta questo dizionario se le tue etichette sono diverse (es. 'CN'/'AD')
            label_mapping = {'Control': 0, 'ProbableAD': 1, 'CN': 0, 'AD': 1}
            
            # Usa la colonna corretta per la diagnosi. Potrebbe chiamarsi 'Dx', 'diagnosis', ecc.
            # Cerchiamo di essere flessibili.
            dx_col = None
            for col in ['Dx', 'diagnosis', 'label']:
                if col in merged_df.columns:
                    dx_col = col
                    break
            
            if dx_col is None:
                 raise ValueError(f"Colonna diagnosi non trovata. Colonne disponibili: {merged_df.columns}")

            # Applica il mapping. Se un valore non è nel dizionario, potrebbe dare errore o NaN.
            try:
                y_true = merged_df[dx_col].map(label_mapping).astype(int)
            except ValueError as e:
                 print(f"Errore durante il mapping delle etichette. Valori unici trovati in {dx_col}: {merged_df[dx_col].unique()}")
                 raise e
            # ---------------------------------------------

            print("----- Valutazione Classificazione -----")
            print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
            print(f"F1-Score (Weighted): {f1_score(y_true, y_pred, average='weighted'):.4f}")
            print("\nClassification Report:")
            print(classification_report(y_true, y_pred, target_names=['CN', 'AD']))
            print("\nConfusion Matrix:")
            print(confusion_matrix(y_true, y_pred))
            print("------------------------------------")
        else: # regression
            y_true = merged_df['MMSE']
            print("----- Valutazione Regressione -----")
            rmse = root_mean_squared_error(y_true, y_pred)
            print(f"RMSE: {rmse:.4f}")
            print("------------------------------------")
===== ./src/__init__.py =====

===== ./scripts/transcribe.py =====
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import torch
import gc

# Aggiunge la root del progetto al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config

# ===================================================================
#      IMPLEMENTAZIONE CORRETTA PER NYRAHEALTH/CRISPERWHISPER
# ===================================================================
def get_crisperwhisper_transcriber(config):
    """
    Prepara la pipeline per CrisperWhisper.
    Carica il token dal file .env solo per questo engine.
    """
    # --- MODIFICA CHIAVE: IMPORT E CARICAMENTO LOCALE ---
    # Questo codice viene eseguito SOLO se l'engine è "crisperwhisper".
    # In questo modo, gli altri ambienti non hanno bisogno di 'python-dotenv'.
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("Info: File .env trovato e caricato per l'autenticazione a Hugging Face.")
    except ImportError:
        print("Info: Libreria 'python-dotenv' non trovata. Si procederà usando il token salvato da 'huggingface-cli login' se disponibile.")
    # --- FINE MODIFICA ---

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    except ImportError:
        print("\nERRORE: La libreria 'transformers' (versione custom) non è installata.")
        print("Assicurati di aver attivato 'crisper_env' e di aver eseguito 'pip install git+...'")
        sys.exit(1)

    cfg = config.transcription.crisperwhisper
    device = config.device
    torch_dtype = torch.float16 if "cuda" in device else torch.float32
    
    print(f"Caricamento backend CrisperWhisper (HF Pipeline) con modello '{cfg.model_id}'...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        cfg.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(cfg.model_id)
    transcription_pipeline = pipeline(
        "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor, chunk_length_s=30,
        batch_size=cfg.batch_size, torch_dtype=torch_dtype, device=device,
    )
    print("Backend CrisperWhisper inizializzato.")

    def transcribe_files(audio_dir, output_dir):
        # ... (Questa logica interna rimane invariata)
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                result = transcription_pipeline(str(audio_path))
                full_text = result["text"].strip()
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files

# ===================================================================
#             LOGICA PER NEMO E WHISPERX (INVARIATE)
# ===================================================================

def get_nemo_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import nemo.collections.asr as nemo_asr
    except ImportError:
        print("\nERRORE: NVIDIA NeMo Toolkit non è installato. Attiva l'ambiente 'nemo_env'.")
        sys.exit(1)
    cfg = config.transcription.nemo
    print(f"Caricamento backend NeMo con modello '{cfg.model_name}'...")
    backend = nemo_asr.models.ASRModel.from_pretrained(model_name=cfg.model_name)
    backend.to(torch.device(config.device))
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o sono già tutti presenti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        str_paths = [str(p) for p in audio_files]
        result = backend.transcribe(audio=str_paths, batch_size=cfg.batch_size, channel_selector=0, verbose=True)
        transcriptions = result[0] if isinstance(result, tuple) else result
        for audio_path, text_obj in zip(audio_files, transcriptions):
            final_text = text_obj.text if hasattr(text_obj, 'text') else text_obj
            out_path = output_dir / f"{audio_path.stem}.txt"
            out_path.write_text(final_text.strip() if final_text else "", encoding='utf-8')
    return transcribe_files


def get_whisperx_transcriber(config):
    # ... (codice identico a prima, nessuna modifica)
    try:
        import whisperx
    except ImportError:
        print("\nERRORE: La libreria 'whisperx' non è installata. Attiva l'ambiente 'whisperx_env'.")
        sys.exit(1)
    cfg = config.transcription.whisperx
    print(f"Caricamento backend WhisperX con modello '{cfg.model_name}'...")
    model = whisperx.load_model(cfg.model_name, config.device, compute_type=cfg.compute_type, language=cfg.language)
    print("Backend WhisperX inizializzato.")
    def transcribe_files(audio_dir, output_dir):
        audio_files = sorted([p for p in Path(audio_dir).rglob("*.wav")])
        if not config.transcription.overwrite:
            audio_files = [f for f in audio_files if not (output_dir / f"{f.stem}.txt").exists()]
        if not audio_files:
            print(f"Nessun file da trascrivere in {audio_dir} (o già trascritti).")
            return
        print(f"Trascrizione di {len(audio_files)} file da {audio_dir}...")
        for audio_path in tqdm(audio_files, desc=f"Transcribing {audio_dir.name}"):
            try:
                audio = whisperx.load_audio(str(audio_path))
                result = model.transcribe(audio, batch_size=cfg.batch_size)
                full_text = " ".join([segment['text'].strip() for segment in result["segments"]])
                out_path = output_dir / f"{audio_path.stem}.txt"
                out_path.write_text(full_text, encoding='utf-8')
            except Exception as e:
                print(f"\nERRORE durante la trascrizione di {audio_path.name}: {e}")
                continue
    gc.collect()
    torch.cuda.empty_cache()
    return transcribe_files


def main():
    # L'import di dotenv è stato rimosso da qui.
    parser = argparse.ArgumentParser(description="Genera trascrizioni per i dati.")
    # ... (il resto della funzione main rimane identico)
    parser.add_argument("--config", type=str, required=True, help="Percorso al file config.yaml")
    args = parser.parse_args()
    config = load_config(args.config)
    engine = config.transcription.engine.lower()
    transcribe_function = None
    if engine == "crisperwhisper":
        transcribe_function = get_crisperwhisper_transcriber(config)
    elif engine == "nemo":
        transcribe_function = get_nemo_transcriber(config)
    elif engine == "whisperx":
        transcribe_function = get_whisperx_transcriber(config)
    else:
        print(f"ERRORE: Engine '{engine}' non supportato. Scegli tra 'crisperwhisper', 'nemo' o 'whisperx'.")
        sys.exit(1)
    model_folder_name = ""
    if engine == "crisperwhisper":
        model_folder_name = config.transcription.crisperwhisper.model_id.split('/')[-1]
    elif engine == "nemo":
        model_folder_name = config.transcription.nemo.model_name.split('/')[-1]
    elif engine == "whisperx":
        model_folder_name = f"WhisperX_{config.transcription.whisperx.model_name}"
    train_output_dir = Path(config.data.transcripts_root) / model_folder_name
    test_output_dir = Path(config.data.test_transcripts_root) / model_folder_name
    train_output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Gli output verranno salvati in: {train_output_dir} e {test_output_dir}")
    print(f"\n--- Inizio Trascrizione Training Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.audio_root), train_output_dir)
    print(f"\n--- Inizio Trascrizione Test Set (engine: {engine}) ---")
    transcribe_function(Path(config.data.test_audio_root), test_output_dir)
    print("\nTrascrizione completata.")

if __name__ == "__main__":
    main()
===== ./scripts/run.py =====
import argparse
import sys
from pathlib import Path

# Aggiunge la root del progetto al path per permettere 'from src import ...'
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.utils import set_seed
from src.data import get_data_splits, get_dataloaders
from src.engine import Trainer, Predictor, Evaluator

def main():
    parser = argparse.ArgumentParser(description="Script principale per training, predizione e valutazione.")
    parser.add_argument("--config", type=str, required=True, help="Percorso al file di configurazione (es. config.yaml)")
    parser.add_argument("--mode", type=str, required=True, choices=["train", "predict", "evaluate"], help="Modalità di esecuzione.")
    args = parser.parse_args()

    config = load_config(args.config)
    set_seed(config.seed)

    if args.mode == 'train':
        print(f"Inizio training per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Gli output verranno salvati in: {config.output_dir}")
        
        # Il K-Fold è gestito qui
        for fold, train_df, val_df, _ in get_data_splits(config):
            train_loader, val_loader = get_dataloaders(config, train_df, val_df)
            trainer = Trainer(config, train_loader, val_loader, fold)
            trainer.train()
            
    elif args.mode == 'predict':
        print(f"Inizio predizione per il task '{config.task}' con la modalità '{config.modality}'.")
        print(f"Uso i modelli da: {config.output_dir}")
        
        # Ottieni il test loader (usiamo un ciclo fittizio per prendere il primo test_df)
        for _, _, _, test_df in get_data_splits(config):
            _, _, test_loader = get_dataloaders(config, train_df=test_df, val_df=test_df, test_df=test_df)
            break # Usciamo dopo il primo, dato che test_df è sempre lo stesso
            
        predictor = Predictor(config, test_loader)
        predictor.predict()

    elif args.mode == 'evaluate':
        print(f"Inizio valutazione per il task '{config.task}'.")
        print(f"Valuto le predizioni in: {Path(config.output_dir) / 'predictions.csv'}")
        
        evaluator = Evaluator(config)
        evaluator.evaluate()

if __name__ == "__main__":
    main()
===== ./config.yaml =====
# ===================================================================
#                    CONFIGURAZIONE PRINCIPALE
# ===================================================================

# --- Impostazioni Generali ---
seed: 42
device: "cuda"  # "cuda" o "cpu"

# --- Percorsi dei Dati ---
data:
  audio_root: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio"
  transcripts_root: "transcripts"
  train_labels: "Adresso21/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv"
  
  test_audio_root: "Adresso21/ADReSSo21-diagnosis-test/ADReSSo21/diagnosis/test-dist/audio"
  test_transcripts_root: "transcripts_test"
  test_task1_labels: "Adresso21/label_test_task1.csv"
  test_task2_labels: "Adresso21/label_test_task2.csv"

# ===================================================================
#                   CONFIGURAZIONE TRASCRIZIONE (ASR)
# ===================================================================
# Modifica 'engine' per scegliere quale motore ASR usare.
# Lo script leggerà solo la sezione di configurazione corrispondente.
# ===================================================================

transcription:
  # CAMBIA QUI: "crisperwhisper", "nemo", o "whisperx"
  engine: "nemo"
  overwrite: false

  # --- SEZIONE CRISPERWHISPER RIVISTA E CORRETTA ---
  crisperwhisper:
    # L'unico parametro che ci serve è l'ID del modello su Hugging Face
    model_id: "nyrahealth/CrisperWhisper" 
    batch_size: 16
    compute_type: "float16" # o "int8"
    
  # --- Impostazioni per NeMo ---
  nemo:
    model_name: "nvidia/parakeet-tdt-0.6b-v2"
    batch_size: 2

  # --- Impostazioni per WhisperX ---
  whisperx:
    model_name: "nyrahealth/faster_CrisperWhisper"
    batch_size: 16
    compute_type: "float16"
    language: "en"
# ===================================================================
#                     CONFIGURAZIONE ESPERIMENTO
# ===================================================================
# Questa sezione controlla le operazioni di training, predizione e valutazione
# lanciate con 'python scripts/run.py'.
# ===================================================================

# --- Task e Modello ---
task: "regression"    # "classification" o "regression"
modality: "text"          # "text" o "audio"

# Modello di trascrizione da usare per l'addestramento del modello testuale.
# Deve corrispondere al nome della cartella creata da 'transcribe.py'.
# Esempio per Crisper: "Crisper_whisper-large-v3"
# Esempio per NeMo: "parakeet-tdt-0.6b-v2"
transcription_model_for_training: "parakeet-tdt-0.6b-v2"

# --- Cross-Validation ---
k_folds: 10
output_dir: "outputs/bert_regression_parakeet" # Cartella dove salvare modelli e predizioni

# --- Parametri del Modello ---
model:
  text:
    name: "bert-large-uncased"
    max_length: 275
    dropout: 0.1

  audio:
    name: "ecapa-tdnn"
    pretrained: "speechbrain/spkrec-ecapa-voxceleb"
    sample_rate: 16000
    trainable_encoder: true 
    dropout: 0.1

# --- Parametri di Addestramento ---
training:
  epochs: 15
  batch_size: 8
  learning_rate: 1.0e-2
  weight_decay: 0.01
  warmup_ratio: 0.1
  early_stopping_patience: 5
  eval_metric: "rmse" # Per classificazione: "accuracy" o "f1". Per regressione: "rmse".
